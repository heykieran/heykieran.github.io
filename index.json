[{"authors":["heykieran"],"categories":null,"content":"Kieran Owens is the CTO of Timpson Gray, a company providing acounting systems to the Private Equity industry.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590756737,"objectID":"1b59167ee93fdb1982478d476b8e75dd","permalink":"https://heykieran.github.io/author/kieran-owens/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kieran-owens/","section":"authors","summary":"Kieran Owens is the CTO of Timpson Gray, a company providing acounting systems to the Private Equity industry.","tags":null,"title":"Kieran Owens","type":"authors"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction Developing software in the Accounting \u0026amp; Finance industry has some peculiar considerations that probably don\u0026rsquo;t exist in other industries. An active and ambitious engineering team\u0026rsquo;s incumbent competition is often not some big, legacy, bespoke system that needs to be replaced or adjusted, while incorporating new features and capabilities: but is often an Excel workbook (or even a number of connected workbooks) where business logic, rules and calculations are captured, and which continuously evolves in an almost organic way.\nAs new needs are identified by the business, an analyst or accountant will fire up Excel, pruning and grafting as necessary, and quickly produce a new version incorporating the business' new requirements.\nIt\u0026rsquo;s a low-friction and efficient approach. The user with the best insight and information works in a tool with which he or she is completely comfortable. That user knows the inputs, knows the calculations, and knows the desired outcomes. What could be better?\nIt\u0026rsquo;s a steep hill for an engineering team to run up to convince anyone that what should properly happen is that the financial-analyst should involve a business-analyst to document the business' requirements, get sign-offs from various stake-holders, schedule some time with the product and engineering teams, who will at some undetermined time in the future produce an artifact that will, most likely, be outdated on the very day on which it arrives.\nYes, that\u0026rsquo;s a very steep hill!\nHowever, there are a number of very sound reasons to approach any maturation or evolution of critical business systems in this precise manner.\nThe legal and compliance teams get a little \u0026ldquo;twitchy\u0026rdquo; when calculations and data that drive critical business decisions live exclusively in constantly evolving Excel workbooks. How do they verify correctness? How can they systematically evaluate the impact of any changes on business operations? How do they even know that the ex ante and ex post outcomes driven by calculations in Excel workbooks differ, and by how much?\nThese are difficult questions to answer and make it hard to evaluate operational risk, often requiring the business to engage in complex hoop-jumping in order to avoid the outcome of crippling innovation and evolution, yet remaining committed to the concepts of sound operational governance. The approaches are many and varied - version control; manual and automated validation; automated forecasting and hindcasting with known inputs and expected outputs. The list goes on.\nA better approach might be to let the analysts and \u0026ldquo;business-heads\u0026rdquo; just go about their business as efficiently as possible, continuing to use Excel as the modelling tool, and only when they are satisfied involve the software developers.\nIf, at that point, it were possible to convert to workbook to source code then the business would be in a better position to assess the impacts of any changes, whether they be in the mode of operation (i.e. calculations) or outcomes (i.e. the results of those calculations).\nA Test Workbook Let\u0026rsquo;s construct a relatively simple Excel workbook that records the scores attained by four individuals on two teams, calculates the totals for each team, and indicates which team won based on which team had the highest score.\nThe Workbook uses only the Excel functions SUMIF, MAX and IF, but the approach would work in concept for any workbook, and is useful for illustrative purposes\n\r\rFig. 1 - A view of our simple Excel Workbook showing precedents and dependents for cell B8.\r\r\rExcel as a Graph An Excel workbook can be expressed as a DAG (directed acyclic graph) where each node is connected to the node(s) on which it depends and that depend on it. Even a node without dependent nodes (e.g. a simple =1+1) can be incorporated in the DAG by connecting it to a synthetic node ($$ROOT) at the root of its worksheet, and that node connected to another singleton node representing the root of the workbook ($$ROOT).\nIf we process the workbook using the code in the accompanying repository, we can produce a visualization of the DAG as follows:\n(-\u0026gt; \u0026quot;SIMPLE-1.xlsx\u0026quot;\r(explain-workbook)\r(get-cell-dependencies)\r(add-graph)\r(connect-disconnected-regions)\r(:graph)\r(uber/viz-graph))\r \r\rFig 2 - The Workbook\u0026rsquo;s DAG including synthetic $$ROOT nodes.\r\r\rWe can see from the DAG produced by the code, that the value of cell B8 (reading left to right) depends on the values of cells C4, C5, C3, B4, B2, B5, C2, B3, and A8, and that its value is also a dependency of cells C8 and C9. This is consistent with the precedents and dependents information that Excel reports, shown in Fig 1.\nWe can also produce a visualization of the DAG that includes each node\u0026rsquo;s attributes by using\n(-\u0026gt; \u0026quot;SIMPLE-1.xlsx\u0026quot;\r(explain-workbook)\r(get-cell-dependencies)\r(add-graph)\r(connect-disconnected-regions)\r(:graph)\r(uber/viz-graph :auto-label true))\r which will display\n\r\rFig 3 - The Workbook\u0026rsquo;s DAG with node details.\r\r\rThe graph, therefore, encodes all the information of the Excel workbook as a convenient Clojure data structure, which can be evaluated or manipulated as required.\nA cell\u0026rsquo;s node attributes contains a :value key storing the cell\u0026rsquo;s value as calculated by Excel, and if it\u0026rsquo;s a formula cell, the text of the formula in the :formula key.\nSo far, so good. Now, the next step is to convert the Excel formulae to Clojure code, and to provide a way to evaluate that code in the proper order, as dictated by the DAG.\n The code below can be found in a Github repository, which contains significantly more detail about the approach taken and the implementation than can be conveniently covered in this post.\n The Excel Graph as Code Using the data-structure calculated in this way, we can then convert each cell\u0026rsquo;s Excel formula to Clojure code and recalculate the workbook in our Clojure REPL according to the DAG: first using the values as supplied in the workbook in order to validate that the conversion results in the same calculated values; and then using other input values to test alternative input scenarios and the workbook\u0026rsquo;s responses to those inputs.\nThis provides a convenient way to both serialize the workbook as Clojure code, and to evaluate the workbook\u0026rsquo;s responses to different inputs.\nIf we run the following from a REPL,\n(-\u0026gt; \u0026quot;SIMPLE-1.xlsx\u0026quot;\r(explain-workbook)\r(get-cell-dependencies)\r(add-graph)\r(connect-disconnected-regions)\r;; recalculate the workbook according to its dependencies,\r;; but using clojure code to perform the calculations\r(recalc-workbook \u0026quot;Scores\u0026quot;)\r(simplify-results))\r it produces the following output showing the Clojure code generated from the Excel formula (in :clj-code); the value as calculated by Excel (:excel-value); and, the value produced by the evaluated Clojure code (:clj-value).\n[{:cell \u0026quot;Scores!B9\u0026quot;\r:formula \u0026quot;SUMIF(B$2:B$5,A9,C$2:C$5)\u0026quot;\r:clj-code (functions/fn-sumif\r(eval-range\r\u0026quot;Scores!B$2:B$5\u0026quot;)\r(eval-range\r\u0026quot;Scores!A9\u0026quot;)\r(eval-range\r\u0026quot;Scores!C$2:C$5\u0026quot;))\r:excel-value 230.0\r:clj-value 230.0}\r{:cell \u0026quot;Scores!B8\u0026quot;\r:formula \u0026quot;SUMIF(B$2:B$5,A8,C$2:C$5)\u0026quot;\r:clj-code (functions/fn-sumif\r(eval-range\r\u0026quot;Scores!B$2:B$5\u0026quot;)\r(eval-range\r\u0026quot;Scores!A8\u0026quot;)\r(eval-range\r\u0026quot;Scores!C$2:C$5\u0026quot;))\r:excel-value 220.0\r:clj-value 220.0}\r{:cell \u0026quot;Scores!C9\u0026quot;\r:formula \u0026quot;IF(B9=MAX(B$8:B$9),\\\u0026quot;WINNER\\\u0026quot;, \\\u0026quot;\\\u0026quot;)\u0026quot;\r:clj-code (if\r(functions/fn-equal\r(eval-range\r\u0026quot;Scores!B9\u0026quot;)\r(functions/fn-max\r(eval-range\r\u0026quot;Scores!B$8:B$9\u0026quot;)))\r(str\r\u0026quot;WINNER\u0026quot;)\r(str\r\u0026quot;\u0026quot;))\r:excel-value \u0026quot;WINNER\u0026quot;\r:clj-value \u0026quot;WINNER\u0026quot;}\r{:cell \u0026quot;Scores!C8\u0026quot;\r:formula \u0026quot;IF(B8=MAX(B$8:B$9),\\\u0026quot;WINNER\\\u0026quot;, \\\u0026quot;\\\u0026quot;)\u0026quot;\r:clj-code (if\r(functions/fn-equal\r(eval-range\r\u0026quot;Scores!B8\u0026quot;)\r(functions/fn-max\r(eval-range\r\u0026quot;Scores!B$8:B$9\u0026quot;)))\r(str\r\u0026quot;WINNER\u0026quot;)\r(str\r\u0026quot;\u0026quot;))\r:excel-value \u0026quot;\u0026quot;\r:clj-value \u0026quot;\u0026quot;}]\r You can see that the values calculated by evaluating the workbook in the REPL are precisely equal to the results calculated by Excel itself.\nAlso, by manipulating the underlying data-structure returned by explain-workbook it is possible to test the responses of the evaluation to different inputs. For example, it is possible, using Clojure, to update the scores for each player and then to recalculate the workbook using those values.\nConclusion The above demonstrates the feasibility of using Clojure\u0026rsquo;s inherent ability to manipulate code data-structures to convert an Excel workbook to Clojure code, and to provide a sound rationale for allowing analysts and accountants to continue to use the tools with which they are most familiar, and to then subsequently take the fruits of their labors and convert them to a form that\u0026rsquo;s systematically comparable, versionable and testable.\n","date":1637620824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637944517,"objectID":"46013e553cd61e16585bd5ef089101fb","permalink":"https://heykieran.github.io/post/excel-to-clojure/","publishdate":"2021-11-22T17:40:24-05:00","relpermalink":"/post/excel-to-clojure/","section":"post","summary":"An approach to \"restating\" an Excel workbook as executable Clojure code in order to verify its calculations; test those calculations against known inputs; compare it with previous versions; and potentially provide a path to retiring the Workbook as *executable content*, reducing the operational risk of exclusively positioning Excel at the core of important business processes.","tags":["clojure","Excel","graph","dag"],"title":"Convert an Excel Workbook to Clojure Code","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction In a ReFrame SPA, it\u0026rsquo;s a relatively easy task to asynchronously initiate a backend process using the :http-xhrio effect handler available in the Reframe library. It\u0026rsquo;s also a rather simple matter of responding to the success and failure events (however they\u0026rsquo;re defined) produced by the back-end process when it finishes. However, what\u0026rsquo;s less obvious is how to capture any system processing messages issued by the backend process as it progresses, but before it sucessfully completes or fails.\nTwo mechanisms available are websockets or SSE. They will both suffice, but each has its own benefits and drawbacks.\nA repo with a working version of this code is available on GitHub (tag v1.4).\nWeb Sockets WebSockets are an interactive, bi-directional (full duplex), out-of-band communication protocol used for sending messages between web servers and clients. Although WebSocket connections use HTTP ports, WebSockets is not an HTTP protocol. It is a separate TCP protocol with its own semantics. WebSockets enable full-duplex communication between a web client and server, without any need for the polling that is demanded by the half-duplex nature of HTTP.\nThe channel is full-duplex and both endpoints (server and client) are immediately aware of any channel changes, such as a disconnection. A problem with this immediate feedback is that both client and server must respond appropriately. Was the channel closed, or are there network issues? Should the client attempt to reopen the channel? etc.\nMost WebSocket implementations also impose a limit to how long they will maintain an open channel which hasn\u0026rsquo;t transmitted or received data recently. Often we would like to use WebSockets with long periods of time between bursts of activity. Therefore, it is the responsibility of the developer to periodically send ping messages along the channel in order to remain below the time-out interval and ensure the connection is maintained.\nSSE (Server Sent Events) Server Sent Event is a push technology which fully exists within the HTTP world (using content type text/event-stream). SSE allows a server to send a stream of events to a web client after the client has established an initial connection. Once a connection has been established the client can use the JavaScript EventSource API to subscribe to the stream, and to receive updates as they are issued by the server. The stream will remain open until explicitly closed by either the server or the client.\nSSE streams are uni-directional (simplex) from the server to the client and unlike WebSockets there is no facility available for the client to use the channel to transmit to the server. Helpfully, SSE provides for automatic client reconnections - convenient when network issues are encountered.\nOn the other hand, one of the challenges of working with SSE is that generally the server is not aware that a client has disconnected until it attempts to send a message. This can be a resource drain for servers with more than a handful of zombie clients. Fortunately, most implementations check the liveness of the connections by automatically issuing periodic heartbeat empty messages to validate the status of the channel.\nBecause both the reconnection and heartbeat features are often part of the base implementation of the SSE protocol, SSE is often simpler in use than WebSockets.\nImplementations Background For the purpose of this exercise, let\u0026rsquo;s suppose we\u0026rsquo;ve created a Clojure function that performs a number of activities that take some non-trivial amount of time. We\u0026rsquo;ve developed its functionality in the REPL and we\u0026rsquo;re satisfied that it\u0026rsquo;s suitable to be made available at a particular Pedestal controlled URL accessible to our front-end application.\nHowever, we would also like to be able to execute this function (via the URL) without stalling our front-end application and to be able to monitor its progress as it proceeds to completion. Ideally, we want to receive processing status update messages from the long-running Clojure function and present them to the user in the browser.\nOf course, in order to avoid stalling the front-end interface we can initiate processing using the :http-xhrio effect handler in ReFrame. However, this will send an AJAX request, executing the function in the background, and to some extent making it inaccessible from our application until it either suceeds or fails.\nThe :http-xhrio effect handler can also take a :progress-handler option, but this isn\u0026rsquo;t really suitable for our current requirements.\nOn the client, the approach we\u0026rsquo;ll take is to provide a pair of React component. One that will be used to receive incoming status messages sent by the server, and another to display those messages in the UI. We will implement these components using both WebSockets and SSE streams as the underlying transports.\n\rIn order to fully understand the login process initiated by the client and the server\u0026rsquo;s response, particularly the fields returned, it\u0026rsquo;s probably helpful to review some details. I\u0026rsquo;ve covered this in a previous post which is available here.\r\r\rClient-Side React Components We will implement two React components to provide the message logging functionality: a log-holder-element-ui component and a log-page-ui component.\n\rThe client side implementation in the repo contains separate pairs of components for the two transport types web-socket and SSE. The former is in the web.logs namespace, the latter in the web.sse-logs namespace.\nApart from some obvious naming diferences in function names, they are more or less interchangable. For this post I will discuss the web-socket implementation. In another post I intend to outline the SSE implementation, and the manner in which it differs from its web socket sibling.\n\r\rComponent #1 - The log-page-ui Component The log-page-ui component is the simpler of the two components. It\u0026rsquo;s primary responsibility is to read the vector of messages stored in ReFrame\u0026rsquo;s state db\u0026rsquo;s :log-messages field and display it in tabular form.\nEach message entry in the vector is a map with keys of :source, :message and message-type. The display component subscribes to the :logs/log-messages subscription, which retrieves the vector, and displays the results. We won\u0026rsquo;t take a detailed look at this component as it\u0026rsquo;s very simple, but you can review it in the accompanying code repo.\nComponent #2 - The log-holder-element-ui Component The log-holder-element-ui component is more complex. It is responsible for populating the ReFrame state db\u0026rsquo;s :log-messages vector. In order to do so, it opens a connection (web-socket or SSE) to the server, reads incoming messages, and transfers them to the state db.\nThe component is also responsible for managing the connection\u0026rsquo;s lifecycle, including any keep-alive requirements and recovery from network errors.\nThe component is a global singleton component and is used to manage and control the SPA\u0026rsquo;s entire logging functionality. It exists throughout the SPA\u0026rsquo;s lifetime, and independently of the user\u0026rsquo;s context in the application, or what action he or she is performing. This simplifies server routing of messages. The server can simply direct them at a client instance. The singleton component will then process the message into the SPA\u0026rsquo;s ReFrame state atom, and the contents can be displayed on demand by the log-page-ui component.\nThe log-holder-element-ui component is implemented, in reagent terms, as a Form-3 component. This is necessary because we require access to React\u0026rsquo;s lifecycle functions. Using these methods we can ensure that our component is a singleton, and that only one instance exists within the application even if the logged-in user changes. The lifecycle methods used (adopting reagent\u0026rsquo;s naming convention) are :component-did-mount, :component-did-update, :component-will-unmount and :reagent-render.\nInternal State The component maintains two internal state variables log-atom and timer-atom.\nThe log-atom contains a map with keys :user-id, :session-id, :connection-uuid and :web-socket, and records an association between an application session identifiers and the web socket servicing that session.\nThe SPA as written uses session- and jwt-based authentication. Because a session is shared between browser tabs, the :connection-uuid value is used to differentiate between tabs within the same session. The :connection-uuid is determined randomly by the client when it first loads.\n\rThere is also an :ext-token entry associated with the user map in the ReFrame db.\nThis is part of the application\u0026rsquo;s JWT security implementation. It is used to provide a way to validate the identity of a user in situations where session authentication is not possible.\nWhen a user logs in to the system the server will generate and return a signed jwt token. This token can be used by the client in subsequent requests to assert a value for the user\u0026rsquo;s identity.\nBecause web sockets exist in the Jetty session layer rather than the Pedestal interceptor layer we cannor rely on the authentication and authorization interceptors I discussed in a previous post to provide server-side security for web-socket connection attempts.\nThe JWT token (ext-token) can be used to provide such security.\n\r\rThe timer-atom is responsible for the keep-alive activities of the connection and will send periodic PING messages to the server, from which it expects a PONG response.\nA websocket is opened by the component by opening a connection to wss://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/ws?session-id=\u0026lt;session-id\u0026gt;\u0026amp;connection-uuid=\u0026lt;connection-uuid\u0026gt;\u0026amp;ext-token=\u0026lt;ext-token\u0026gt; on the server.\nThe session-id value is the session identifier requested by the client, which is correlated with the browser session. Therefore, different tabs will share the same session-id because they share the same browser session.\nAs mentioned above the connection-uuid value is a value generated by the client to identify a particular instance of the application (i.e. browser tab) and the ext-token value is the jwt token returned by the server when a user logs in (see note above).\nInternally, the server will use the session and connection identifiers to direct messages to the appropriate target client.\nBy convention, the session-id value is a combination of the user\u0026rsquo;s application user-id (a keyword) and a unique integer e.g. :user-12. The incorporation of the user-id in the session-id in this manner simplifies the processing of certain security restrictions imposed by the server.\nLifecycle Methods :component-did-mount is responsible for initializing the :timer-atom and starting the keep-alive process.\n:component-did-update checks if either the current use has logged out, in which case it disconnects the websocket; or if a new user logs in, in which case it creates and connects a new websocket, clears the log messages stored in ReFrame\u0026rsquo;s state db, and finally initializes the internal log-atom and associates the log session\u0026rsquo;s identifiers with the websocket.\ncomponent-will-unmount stops the keep-alive process by clearing the the timer-atom and the log-atom.\n:reagent-render returns a simple empty DIV element, which seems odd. However, this element is inserted in the static portion of the SPA\u0026rsquo;s main page and created only once when the SPA is loaded.\nThe implementation of the log-holder-element-ui component is included below. The :component-did-update lifecycle method checks whether the user is logging out i.e. there exists a current session (existing-session-id) and the new-session-id value is nil; or whether a new user has logged in.\n(defn log-holder-element-ui\r[{session-id :client-id\ruser-id :user\rext-token :ext-token\rconnection-uuid :connection-uuid\r:as current-logged-in-user}]\r(let\r[timer-atom (atom nil)\rlog-atom (atom nil)]\r(if connection-uuid\r(ws-utils/connect-ws-for-log\ruser-id\rsession-id\rconnection-uuid\rext-token\rlog-atom)\r(println \u0026quot;No connection-uuid available for logging element \u0026quot;\r\u0026quot;during creation of component\u0026quot;))\r(reagent/create-class\r{:display-name\r:alloc-log-component\r:component-did-mount\r(fn[this]\r(println \u0026quot;Log Component didMount\u0026quot;)\r(reset!\rtimer-atom\r(js/setInterval\r(fn[]\r(if @log-atom\r(ws-utils/send-ping\rlog-atom)))\r(* 1000 60 2))))\r:component-did-update\r(fn[this old-argv]\r(println \u0026quot;Log Component didUpdate\u0026quot;)\r(let [new-argv (rest (reagent/argv this))\r{new-session-id :client-id\rnew-connection-uuid :connection-uuid\rnew-user-id :user\rnew-ext-token :ext-token\r:as current-logged-in-user}\r(first new-argv)\r{existing-user-id :user\rexisting-session-id :client-id}\r(first (rest old-argv))]\r(if (and (nil? new-session-id) existing-session-id)\r(do\r(println\r(str \u0026quot;Clearing websocket for user \u0026quot;\r(pr-str existing-user-id) \u0026quot;, \u0026quot;\r\u0026quot;with client-id \u0026quot;\r(pr-str existing-session-id)))\r(ws-utils/disconnect-web-socket-for-log log-atom)\r(reset! log-atom nil)))\r(if new-session-id\r(do\r(println\r(str \u0026quot;Creating websocket for user \u0026quot;\r(pr-str new-user-id)\r\u0026quot;with client-id \u0026quot;\r(pr-str new-session-id)))\r(ws-utils/connect-ws-for-log\rnew-user-id\rnew-session-id\rnew-connection-uuid\rnew-ext-token\rlog-atom)))))\r:component-will-unmount\r(fn[this]\r(println \u0026quot;Log Component willUnmount\u0026quot;)\r(js/clearInterval @timer-atom)\r(reset! timer-atom nil)\r(reset! log-atom nil))\r:reagent-render\r(fn[{session-id :client-id :as current-logged-in-user}]\r[:div])})))\r The code above uses a number of utility functions to manage the WebSocket connection. We will taken a deeper look at some of them below.\nThe Web Socket Libary The WebSocket library used by our implementation is haslett.\n(require '[haslett.client :as ws])\r(require '[haslett.format :as fmt])\r Connecting Connections are handled by the connect-ws-for-log function, which is passed a user-id, session-id, connection-uuid and ext-token.\nIt is also passed (in log-atom) the internal state atom of the log-holder-element-ui component.\n(defn connect-ws-for-log\r[user-id session-id connection-uuid ext-token log-atom]\r(println \u0026quot;connecting logging websocket for \u0026quot;\r\u0026quot;user \u0026quot; (pr-str user-id) \u0026quot;, with client-id \u0026quot;\r(pr-str session-id) \u0026quot;and connection-uuid \u0026quot;\r(pr-str connection-uuid))\r(let [{existing-websocket :web-socket\rexisting-user-id :user-id\rexisting-session-id :session-id\rexisting-connection-uuid :connection-uuid}\r@log-atom]\r(when existing-websocket\r(if (ws/connected? existing-websocket)\r(do\r(println\r(str \u0026quot;Web Socket is currently connected. \u0026quot;\r\u0026quot;Closing existing connected websocket for \u0026quot;\r(keyword (name existing-user-id)\r(name existing-session-id))\r\u0026quot; with connection-uuid \u0026quot;\r(pr-str existing-connection-uuid)))\r(ws/close existing-websocket))\r(println\r(str \u0026quot;Existing websocket to \u0026quot;\r(keyword (name existing-user-id)\r(name existing-session-id)) \u0026quot; \u0026quot;\r\u0026quot;exists, but it is not connected.\u0026quot;)))))\r(go\r(let\r[web-socket\r(\u0026lt;! (ws/connect\r(str\rapi-urls/base-websocket-url\r\u0026quot;?session-id=\u0026quot; session-id\r\u0026quot;\u0026amp;ext-token=\u0026quot; ext-token\r\u0026quot;\u0026amp;connection-uuid=\u0026quot; connection-uuid)\r{:format fmt/transit}))]\r(swap!\rlog-atom\r(fn[o n]\r(js/console.log\r\u0026quot;Resetting log details to \u0026quot;\r(pr-str (select-keys n [:user-id :session-id :connection-uuid]))\r\u0026quot; from \u0026quot;\r(pr-str (select-keys o [:user-id :session-id :connection-uuid])))\rn)\r{:user-id user-id\r:session-id session-id\r:connection-uuid connection-uuid\r:web-socket web-socket\r:ext-token ext-token})\r(go\r(loop []\r(when-let\r[msg (\u0026lt;! (:source web-socket))]\r(add-message-to-log msg)\r(recur))))\r(when-let\r[msg (\u0026lt;! (:close-status web-socket))]\r(js/console.log\r\u0026quot;close-status message received -\u0026gt; \u0026quot; (pr-str msg))\r(decide-and-restart\rmsg\ruser-id\rsession-id\rconnection-uuid\rext-token\rlog-atom)))))\r The function first conducts a number of sanity checks, and then, using haslett\u0026rsquo;s connect function, creates a connection to the url on the server. The result of the call to connect is stored in the state atom that was passed as an argument.\nWe use Haslett\u0026rsquo;s connect function to create the underlying WebSocket, and specify that the communication format should be transit. The connect function returns a promise channel that will create a map containing four elements: a :socket, a :source, a :sink and a :close-status.\nThe :socket entry will contain the WebSocket instance that was created. The :source and :sink keys are core.async channels that we will use for reading and writing to the underlying web socket using Clojure\u0026rsquo;s familiar channel metaphors.\nInformation about the connection is stored in the log-atom which is a stateful part of the log-holder-element-ui component. The information stored is all the information supplied by the client to the server and the websocket instance.\nThe connect-ws-for-log function then starts the messages processing loop in a go block.\nWithin the go block, as each message is received on the :source channel connected to the WebSocket, the function add-message-to-log is called and passed the message\u0026rsquo;s contents as an argument. The add-message-to-log function inserts the message in ReFrame\u0026rsquo;s state db.\nAfter the message reading go loop there is a section of code that responds to close events received in the haslett websocket\u0026rsquo;s :close-status channel. When a close message event is received, the decide-and-restart function is called. This function may, depending on the nature of the close event, decide to restart the web socket or not.\nDisconnecting The disconnect-web-socket-for-log function, used when we\u0026rsquo;re shutting down the connection, is also passed the state atom. It closes the WebSocket.\n(defn disconnect-web-socket-for-log\r[log-atom]\r(let\r[{web-socket :web-socket\ruser-id :user-id\rsession-id :session-id\rconnection-uuid :connection-uuid} @log-atom]\r(if (and session-id connection-uuid)\r(do\r(println\r(str \u0026quot;Disconnecting web socket associated with \u0026quot;\r(pr-str user-id) \u0026quot;/\u0026quot; (pr-str session-id)\r\u0026quot; with connection uuid \u0026quot;\rconnection-uuid))\r(if (ws/connected? web-socket)\r(ws/close web-socket)\r(println \u0026quot;Unable to close web socket. It's not connected.\u0026quot;)))\r(println \u0026quot;No session id available. Declined to issue close().\u0026quot;))))\r Web Socket Keep-Alive (Client) A browser will close a WebSocket if no traffic is seen in some particular interval of time (typically 5 minutes). However, status updates often occur in bursts with long periods of inactivity between these bursts. When the log-holder-element-ui component is mounted we start a loop (using js/setInterval) which calls send-ping, a function that places (at two minute intervals) a ping message on the channel connected to the WebSocket.\nThis let\u0026rsquo;s the server know that it shouldn\u0026rsquo;t close the connection.\n(defn send-ping[log-atom]\r(let\r[{web-socket :web-socket\ruser-id :user-id\rsession-id :session-id\rext-token :ext-token\rconnection-uuid :connection-uuid} @log-atom]\r(if session-id\r(do\r(println\r(str \u0026quot;Sending Ping for \u0026quot;\r(pr-str user-id) \u0026quot;/\u0026quot;\r(pr-str session-id)\r\u0026quot; at \u0026quot; (pr-str connection-uuid)))\r(if (ws/connected? web-socket)\r(go\r(\u0026gt;! (:sink web-socket)\r{:asys/ping user-id\r:asys/session-id session-id\r:asys/connection-uuid connection-uuid}))\r(do\r(println\r(str \u0026quot;PING: No websocket connected. \u0026quot;\r\u0026quot;Attempting reconnect for \u0026quot;\r(pr-str user-id) \u0026quot;/\u0026quot;\r(pr-str session-id) \u0026quot;, \u0026quot;\r\u0026quot;at \u0026quot; (pr-str connection-uuid)))\r(connect-ws-for-log\ruser-id\rsession-id\rconnection-uuid\rext-token log-atom))))\r(println \u0026quot;No session id available. No ping sent.\u0026quot;))))\r When the server receives a ping it should respond with a pong. If a web socket isn\u0026rsquo;t connected, and the client expects it to be, the send-ping will attempt to reopen the connection. This may occur when connectivity is lost and the decide-and-restart function has declined to reconnect. A consequence of this approach is that if the server goes away for any reason, the client will attempt to reconnect forever.\nThe Server Side Now we turn our attention to how web socket connections are handled by the server.\nDesign Decisions In order to decouple the underlying transport mechanism (e.g. WebSocket or SSE) from the Server\u0026rsquo;s higher-level message creation and dispatch functions we first create a single core.async channel to which we can write our messages destined for the client.\nWe also create a publication of this channel using a selector of :topic on the received message map. A helpful side-effect of this approach is that a publication of a topic without a matching subscription is simply dropped. The topic we\u0026rsquo;ll use for our log messages is :log-msg. Therefore, a message pushed to our channel with the form {... :topic :log-msg ...} will be forwarded to our publication.\nFinally, we create a subscription to the topic :log-msg, and a servicing function to remove messages from it and forward them to the client using whatever transport mechanism is desired. This decoupling serves two purposes: it allows us to change or update our messaging transport without unnecessarily impacting the server\u0026rsquo;s code; and adding other topics is a relatively simple extension.\nWe also need an atom to store the Server\u0026rsquo;s active subscription\u0026rsquo;s channel.\nCreating the Channel \u0026amp; Publication Below is the code that creates the single message channel, its associated publication and the atom used to store the subscription\u0026rsquo;s channel.\nThe atom will be initialized correctly at application startup time.\n(defonce\rserver-messages-channel-destined-for-all-clients\r(atom nil))\r(defonce\rserver-messages-channel\r(chan 100))\r(defonce\rserver-messages-publication\r(pub server-messages-channel #(:topic %)))\r Starting Pedestal with WebSocket Support In order for Pedestal/Jetty to start with WebSocket support you must, within its service map\u0026rsquo;s ::http/container-options entry, supply a value for the :context-configurator key.\nThe value should be a reference to a function that configures the Jetty Servlet\u0026rsquo;s web-socket behavior for Pedestal. The function should at least call the add-ws-endpoints function in the io.pedestal.http.jetty.websockets namespace. In our application this function is server.messaging.websocket/websocket-configurator-for-jetty shown below.\n(defn websocket-configurator-for-jetty\r[jetty-servlet-context]\r(ws/add-ws-endpoints\rjetty-servlet-context\rws-paths))\r The add-ws-endpoints function takes as parameters a ServletContext and a configuration map. The configuration map passed indicates the websocket uri(s) that Jetty should use as web sockets end-points, and also the functions that should be called when the web socket service receives the on-connect, on-text, on-error and on-close events.\nThe configuation map used by the applications is shown below\n{\u0026quot;/ws\u0026quot;\r{:on-connect\r(ws/start-ws-connection\rnew-ws-client)\r:on-text\r(fn [raw-msg]\r(process-incoming-text-message raw-msg))\r:on-binary\r(fn [payload offset length]\r(process-incoming-binary-message\rpayload offset length))\r:on-error\r(fn [error]\r(process-error error))\r:on-close\r(fn [num-code reason-text]\r(process-close\rnum-code reason-text))}}\r The on-connect handler uses Pedestal\u0026rsquo;s web-socket functionality to start the connection. Pedestal\u0026rsquo;s start-ws-connection function takes as its single argument a function that should accept two parameters passed to it by Pedestal: a websocket (of type org.eclipse.jetty.websocket.api.Session) and an async channel connected to the web socket.\n(defn new-ws-client\r[^Session ws-session send-ch]\r(log/info\r(str \u0026quot;new-ws-client: creating new websocket to client with \u0026quot;\r\u0026quot;uri: \u0026quot; (str (.getRequestURI ^Session ws-session))))\r(let\r[ws-endpoint\r(get-ws-endpoint-from-session ws-session)\rquery-string-map\r(some-\u0026gt; ws-session\r(.getRequestURI)\r(.getQuery)\r(route/parse-query-string))\r[session-id ext-token connection-uuid]\r(as-\u0026gt;\rquery-string-map v\r(mapv #(get v %) [:session-id :ext-token :connection-uuid]))]\r(if (and session-id connection-uuid)\r(let\r[user-id-from-token\r(or\r(auth-utils/get-id-from-ext-token ext-token)\r:anonymous)\rmessage-text (str \u0026quot;new-ws-client: starting web socket \u0026quot;\r\u0026quot;with user \u0026quot; (pr-str user-id-from-token) \u0026quot; \u0026quot;\r\u0026quot;for session-id \u0026quot; session-id \u0026quot; \u0026quot;\r\u0026quot;with connection-uuid \u0026quot; (pr-str connection-uuid) \u0026quot; \u0026quot;\r\u0026quot;from \u0026quot; (pr-str ws-endpoint))\rmessage (value-\u0026gt;transit-string\r{:time (gen-utils/get-local-timestamp-with-offset)\r:text message-text})]\r(log/info message-text)\r(async/put!\rsend-ch\rmessage)\r(swap! ws-clients\rassoc\r(keyword\r(name (gen-utils/possible-string-as-keyword user-id-from-token))\r(str connection-uuid))\r[ws-session send-ch\r(keyword\r(name (gen-utils/possible-string-as-keyword user-id-from-token))\r(name (gen-utils/possible-string-as-keyword session-id)))]))\r(log/warn (str \u0026quot;No session-id and connection-uuid \u0026quot;\r\u0026quot;supplied for websocket creation.\u0026quot;\r\u0026quot;Websocket not created.\u0026quot;)))))\r Because web sockets exist in the Jetty session layer rather than the Pedestal routing layer we cannot rely on the authentication and authorization interceptors I discussed in a previous post to provide security for connection attempts at the server\u0026rsquo;s /ws url. Therefore, before a client is allowed to establish a connection with a particular session-id (which encode the user\u0026rsquo;s id) the jwt token, passed as a query parameter in the connection url, is examined to determine the user\u0026rsquo;s id.\nWe can extract the client’s requested session-id from the url used by the client to request a web-socket connection.\nAs described earlier, the form of the url is wss://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/ws?session-id=\u0026lt;session-id\u0026gt;\u0026amp;connection-uuid=\u0026lt;connection-uuid\u0026gt;\u0026amp;ext-token=\u0026lt;ext-token\u0026gt;.\nThe ws-clients atom is a record of the currently connected web-socket clients. It contains a map keyed by each client’s session-id with values that is a vector of the client’s Session instance and its connected channel.\nIf the user has a valid user id the connection is allowed.\nIn new-ws-client we extract the session-id requested by the client, sends a short you\u0026rsquo;re connected message, and add a vector describing the connection to the ws-clients atom using a key composed of the user\u0026rsquo;s id and the connection-uuid.\nYou\u0026rsquo;ll notice in the on-text handler function how the server\u0026rsquo;s side of the web socket\u0026rsquo;s keep-alive functionality is implemented in process-incoming-text-message.\n(defn process-incoming-text-message\r[raw-msg]\r(let\r[message (transit-string-\u0026gt;value raw-msg)]\r(log/info \u0026quot;Websocket message received \u0026quot;\r(pr-str message))\r(if\r(and\r(map? message)\r(contains? message :asys/ping))\r(let\r[{user-id :asys/ping\rsession-id :asys/session-id\rconnection-uuid :asys/connection-uuid} message]\r(send-messages-to-clients\r{:msg\r{:asys/pong user-id\r:asys/session-id session-id\r:asys/connection-uuid connection-uuid}\r:target-client\r(keyword\r(name user-id)\r(str connection-uuid))}))\r(log/warn\r(str \u0026quot;Unexpected websocket message received: \u0026quot;\r(pr-str message) \u0026quot;, type: \u0026quot;\r(type message))))))\r If the server receives a message with an :asys/ping key, the server automatically responds with an :asys/pong message. The value of the :asys/ping entry is the user id, session identifier and connection uuid of the client that sent the ping message and enables the server to direct its response correctly.\nStarting the Message Processing When the application starts the web-server, the message-transport function is also called to initialize the application\u0026rsquo;s messaging functionality. This function takes two parameters, a transport-type (which can be either :web-socket or :sse) and the atom used to store the subscription\u0026rsquo;s channel.\n(defn message-transport\r[transport-type message-channel-atom]\r(start-processing-sub transport-type message-channel-atom))\r The message-transport function calls start-processing-sub which is the function that starts the go loop - accepting messages from the subscription channel and forwarding them to the client(s).\n(defn start-processing-sub\r[transport-type message-channel-atom]\r;; If the channel already exists, then close it.\r(when-let [ch (deref message-channel-atom)]\r(async/close! ch))\r;; subscribe a channel to be contained in the\r;; server-messages-channel-destined-for-all-clients atom\r;; to the server-messages-publication with the topic of\r;; :log-msg i.e. anything in the publication with key of\r;; :topic and a value of :log-msg\r(async/sub\rrlog/server-messages-publication\r:log-msg\r(reset! message-channel-atom (async/chan)))\r;; start a go-loop that takes messages off the channel in the\r;; atom and pass it to the send-messages-to-clients function\r(log/info \u0026quot;Starting \u0026quot; (pr-str transport-type) \u0026quot; log message loop\u0026quot;)\r(async/go\r(loop []\r(when-let\r[log-msg (async/\u0026lt;! (deref message-channel-atom))]\r(send-messages-to-clients transport-type log-msg)\r(recur)))))\r The send-messages-to-clients function is defined as a multi-method which selects on the transport-type value and makes sure that the correct send- function(s) are called for the selected transport type.\nEssentially, this gets reduced to calling the send-messages-to-client function in either the server.messaging.websocket or the server.messaging.sse namespaces.\nSending Messages If we review the send-message- functions in the server.messaging.websocket namespace, we can see how they work. (The implementation for SSE is similar, and simpler).\nThe send-messages-to-clients function is the namespace\u0026rsquo;s main public function. It accepts as an argument a map representing the message to be sent. Within the map is an entry :target-client which indicates the message\u0026rsquo;s destination. The function handles broadcast messages, i.e. messages destined for all connected clients, and also messages destined for only one client. In either case, this function will call the private function send-message-to-client in the same namespace as many times as is necessary.\n(defn send-messages-to-clients\r[{message :msg\rtarget-client :target-client\rmessage-type :message-type\r:as whole-message}]\r(if (and (some? target-client)\r(not= :none target-client))\r(do\r(log/info\r(str \u0026quot;websocket: asked to send message to clients \u0026quot;\r(pr-str target-client)\r\u0026quot;, msg: \u0026quot;\r(pr-str whole-message)))\r(if (= :all target-client)\r(doseq\r[target-client (keys @ws-clients)]\r(send-message-to-client\rtarget-client\rmessage\rmessage-type))\r(send-message-to-client\rtarget-client\rmessage\rmessage-type)))))\r The send-message-to-client is shown below. It take a session-id indicating the message\u0026rsquo;s destination and a message which is a map.\n(defn- send-message-to-client\r[connection-identifier message \u0026amp; [message-type]]\r(log/info\r(str \u0026quot;Asked to send web-socket message to individual client \u0026quot;\r(pr-str connection-identifier)\r\u0026quot;, msg is \u0026quot;\r(pr-str message)))\r(if-let\r[ws-connections (get-ws-conns-for-session-id\r@ws-clients\rconnection-identifier)]\r(doall\r(map\r(fn[connection-key]\r(if-let\r[[ws-session send-ch combined-session-id]\r(get @ws-clients connection-key)]\r(if (or\r(not (.isOpen ws-session))\r(ap/closed? send-ch))\r(do\r(log/warn\r\u0026quot;While trying to send-message-to-client,\u0026quot;\r\u0026quot;found websocket or websocket channel was closed.\u0026quot;\r\u0026quot;combined-session-id\u0026quot; (pr-str combined-session-id)\r\u0026quot;connection-key \u0026quot; (pr-str connection-key))\r(clean-up-ws-clients))\r(async/put!\rsend-ch\r(value-\u0026gt;transit-string\r{:time (gen-utils/get-local-timestamp-with-offset)\r:text message\r:message-type (or message-type :info)})\r(fn[v]\r(log/debug\r(str\r\u0026quot;put! in send-message-to-client \u0026quot;\r(pr-str connection-key)\r\u0026quot; returned \u0026quot;\r(pr-str v))))))\r(log/warn (str \u0026quot;Couldn't find websocket session \u0026quot;\r\u0026quot;for client connection \u0026quot;\r(pr-str connection-key)\r\u0026quot;. Available connections \u0026quot;\r\u0026quot;are \u0026quot; (pr-str (keys @ws-clients))))))\rws-connections))\r(log/warn \u0026quot;Couldn't find any web sockets for target-client\u0026quot;\r(:target-client message))))\r Connecting the long-running Clojure function Now that the messaging infrastructure is in place we\u0026rsquo;ll turn our attention to \u0026ldquo;instrumenting\u0026rdquo; our clojure function to send status update message.\nA convenient, but not the only, approach is to leverage Clojure\u0026rsquo;s logging functionality. Often, within a Clojure we will often use calls to log/info and log/debug calls to mark and record important processing events. We could take the opportunity to selectively forward some of these messages to the remote client giving it feedback similar to what might be seen if the function was run in a REPL.\nThis is the approach I\u0026rsquo;ve chosen.\nFirst I create a macro that can wrap a call to functions in log namespace, but which can also accept a session-id.\n(defmacro with-forward-context\r([body]\r(list `with-forward-context nil body))\r([target-id body]\r(list `with-forward-context target-id body {}))\r([target-id body options]\r(list 'do body\r(list\r`apply\r`write-message-to-server-message-channel\r(concat\r(list\r'list\r(list 'clojure.string/join \u0026quot; \u0026quot;\r(conj\r(map\r#(if\r(instance? java.lang.Throwable %)\r\u0026quot;ERROR\u0026quot;\r(list\r'clojure.string/trim\r(list\r`str %)))\r(rest body))\r'list)))\r(if target-id\r(list target-id)\r'())\r(if options\r(list options)\r'()))))))\r Then we can selectively wrap any log calls we like as follow:\n(rlog/with-forward-context\rsession-id\r(log/info \u0026quot;OK\u0026quot;))\r The result is that the message is logged in the usual fashion, but a call to write-message-to-server-message-channel is also made, ensuring that the content of the log message is also sent to the client matching session-id.\nClosure Hopefully, if you\u0026rsquo;re attempting to use websockets with your ClojureScript SPA this has been helpful in some small way. In a subsequent post I will discuss implementing a similar for of messaging but using SSE.\n","date":1591556400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592945475,"objectID":"2912a8334e311f422211a2c332676abf","permalink":"https://heykieran.github.io/post/using-sse-and-websockets/","publishdate":"2020-06-07T15:00:00-04:00","relpermalink":"/post/using-sse-and-websockets/","section":"post","summary":"For long-running server processes possibly initiated by an AJAX call from an SPA client, it's often desirable to be able to receive (and display) status update messages (issued by the server during processing) in your front-end application's UI. In this piece I will discuss the two options of using **websockets** and **SSE** (server sent events), and, with code, show how to implement them using Clojure and ClojureScript.","tags":["clojure","clojurescript","reframe","SSE","websockets"],"title":"Asynchronous communication streams between a Pedestal server and a ReFrame SPA","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction In two earlier posts (available here and here) I showed how to use the buddy-auth library to develop an SPA that uses session-based (or session cookie) security. The SPA accompanying that post (and available on Github tag v1.0) was also capable of using Google credentials to authenticate the user.\nIn this post I will provide further details on how the Google web authentication works, and I will show how to add additional authentication mechanisms (jwt and Google IAP) by developing a custom backend for use with buddy-auth.\nClient-Side Google Sign-In \rCredit where credit is due: The client-side code is based on excellent work done by Tristan Straub which is available on Github.\r\r\rGAPI is a library provided by Google that enables browser-hosted javascript code to call Google API\u0026rsquo;s. It is available at https://apis.google.com/js/platform.js.\nAmongst other functionality, it also includes the code we\u0026rsquo;ll need to interact with the endpoints used by Google Sign-In, which is used to generate the OAuth 2.0 credentials for the user.\nOn the client-side we\u0026rsquo;ll first need to ensure that the GAPI library is loaded. This is done using the \u0026lt;ensure-gapi! function.\nThe \u0026lt;ensure-gapi! function first checks if the GAPI library has already been loaded. If not, it will call the function \u0026lt;load-script to do so.\n(defn- \u0026lt;ensure-gapi!\r[client-id]\r(go\r(when-not (exists? js/gapi)\r(\u0026lt;!\r(\u0026lt;load-script\rconfig/google-script-location)))\r(\u0026lt;! (\u0026lt;init-gapi! client-id))))\r \u0026lt;load-script is an interesting function. It first creates an HTML \u0026lt;script\u0026gt; element in memory, and sets its source attribute to https://apis.google.com/js/platform.js. It then create a promise (as a channel using \u0026lt;cb) to load the javascript file, adds the \u0026lt;SCRIPT\u0026gt; element to the DOM, and returns the channel/promise.\nAfter loading the GAPI libraries, the \u0026lt;ensure-gapi! function calls \u0026lt;init-gapi! to initialize the auth2 components of the library, and to create the GoogleAuth object. These are the components that we\u0026rsquo;ll use to get an OAuth 2.0 credentials claim from Google on behalf of a user.\nOn the SPA\u0026rsquo;s Users page, we can display a styled Google Sign-In button by calling the gapi.signin2.render function. This function takes a DOM element, within which it will render the sign-in button, and a map of parameters.\n(js/gapi.signin2.render\rel\r#js {\u0026quot;scope\u0026quot; \u0026quot;profile email\u0026quot;\r\u0026quot;width\u0026quot; 144\r\u0026quot;height\u0026quot; 30\r\u0026quot;longtitle\u0026quot; false\r\u0026quot;theme\u0026quot; \u0026quot;dark\u0026quot;\r\u0026quot;onsuccess\u0026quot; on-success\r\u0026quot;onfailure\u0026quot; on-failure})\r The scope parameter in the options map sets the user identity elements we\u0026rsquo;d like returned by Google, and on-success and on-failure are Clojurescript functions called according to whether the authentication attempt succeeded or failed.\nWhen the user clicks on the Sign-In button, a second browser window will open to a secure Google URL where the user can enter his or her credentials. If the authentication is successful the on-success function is called with an instance of GoogleUser.\nThen, by calling GoogleUser.getAuthResponse we can retrieve the AuthResponse map; and from that we can get the id_token, which is the ID token granted by Google.\n(fn [google-user]\r(println \u0026quot;running google-login-button on-success function\u0026quot;)\r(let\r[token (some-\u0026gt;\rgoogle-user\r(ocall \u0026quot;getAuthResponse\u0026quot;)\r(oget \u0026quot;id_token\u0026quot;))]\r(when token\r(\u0026lt;post-to-server\rtoken))))\r Given our requested scopes, the AuthResponse will also contain the user\u0026rsquo;s Google email address. However, this isn\u0026rsquo;t to be trusted. The correct (and secure) way to confirm the email address is to validate the ID token using server-side code, before extracting the email address.\nIn order to do this the success function submits the token to the server for validation using an Ajax POST request.\n(defn- \u0026lt;post-to-server\r[token]\r(let\r[form-data\r(str \u0026quot;idtoken=\u0026quot; token \u0026quot;\u0026amp;connection-uuid=\u0026quot; (random-uuid))]\r(POST\rconfig/google-callback-url\r{:body form-data\r:response-format :raw\r:with-credentials true\r:handler (fn[r]\r(js/console.log \u0026quot;Server Response from Google Login\u0026quot;)\r(js/console.log r)\r(js/console.log \u0026quot;will confirm with server\u0026quot;)\r(rf/dispatch\r[:auth/get-logged-in-user]))\r:error-handler (fn[e]\r(js/console.log \u0026quot;Server Error\u0026quot;)\r(js/console.log e))})))\r In our application the address stored in config/google-callback-url is https://\u0026lt;server-name\u0026gt;/auth/google which is a Pedestal route configured, at the end of its interceptor chain, to call the alloc-auth-google-login handler function.\nWithin this function a call is made to verify-google-token-response, passing the POST request\u0026rsquo;s idtoken value. The verify function uses the com.google.api.client.googleapis.auth.oauth2 java library to validate the token and extract the email address.\nThe server will then create an identity-token as explained here, and add it to the Ring session\u0026rsquo;s :identity field.\nFrom this point, the client/server behaviors are as they would be for other session-based authentication types.\nGoogle IAP Google IAP (Identity Aware Proxy) is a context-aware reverse proxy service available within the Google Cloud product line. It sits between the internet and your application\u0026rsquo;s resources and ensures that access to those resources is granted only to users who have the appropriate Cloud IAM role(s).\nWhen a new user attempts to access a resource protected by IAP, IAP will direct the user to a Google OAuth 2.0 sign-in flow where the user can enter his or her credentials. If those credentials are correct, IAP will then assess whether the user is authorized to access the resource by checking the user\u0026rsquo;s roles and permissions. If everything checks out, the request is forwarded to the resource with the user\u0026rsquo;s JWT claim token added to the request\u0026rsquo;s headers in the x-goog-iap-jwt-assertion field.\nIt is the responsibility of the application to verify the validity of this token using Google\u0026rsquo;s public keys.\nFrom the user\u0026rsquo;s perspective, the sign-in flow is similar to the flow used by the client-side sign-in described above, but with the result of a sucessful authentication being the addition of the x-goog-iap-jwt-assertion header and the token to the request\u0026rsquo;s headers rather than calling the on-success function in the browser.\nIAP uses one of a number of different keys to sign the header. The public component of these keys is published (in JWK format) at https://www.gstatic.com/iap/verify/public_key-jwk. As far as I can tell, there is no way to determine which of the available keys Google will use to sign the JWT claim, although the signed token will include in its :kid field the id of the key used to sign it.\nThe Challenge Session backends work just fine and, if handled well, provide perfectly adequate security. In my earlier post I showed how it is possible to integrate Google web login with our Reframe SPA, and although the authentication was performed by Google the end result is that the validated Google identity was inserted into the client\u0026rsquo;s session.\nHowever, there are circumstances in which session authentication \u0026amp; authorization will not work - for example when a browser session doesn\u0026rsquo;t exist. This may occur when making calls to HTTP API endoints from clients other than a browser e.g. server calls, calls from desktop clients, or calls mediated by service orchestration engines.\nIn such cases, the use of authorization bearer tokens, such as JWT, is a better approach. The buddy-auth library comes with a number of included backends, with one of which that can use JWT tokens. However, its implementation assumes that the JWT token will be included in the Authorization header of the request. If our application is using IAP this won\u0026rsquo;t be the case. Therefore we\u0026rsquo;ll need to provide our own backend.\nBuddy Backends A Buddy backend is simply an instance of an object that implements two protocols: IAuthentication and IAuthorization. Our custom backend (jws-embedded) which implements these protocols can be found in the server.auth.header namespace and mimics closely the jws backend provided by Buddy.\nIAuthentication The IAuthentication protocol defines two member functions: -parse and -authenticate. The former is responsible only for inspecting the request and parsing from it the material to be passed to -authenticate. The standard implemenation of the Buddy middleware, initiated in the interceptor returned by our alloc-auth-authentication-interceptor function (taking a backend as a parameter) ensures that if the call to -parse in the backend returns nil, then the backend\u0026rsquo;s -authenticate function won\u0026rsquo;t be called.\nIAuthorization The IAuthorization protocol requires only a single function, -handle-unauthorized which will be called with parameters of the request and some metadata when an authenticated request is not authorized to access a resource.\nRemember the alloc-auth-authentication-interceptor function, which takes a backend as a parameter, creates an interceptor whose :enter method adds to the received context a key :auth/backend and attaches the backend parameter as its value. The :enter function updates the :request map using the buddy.auth.middleware/authentication-request function with the backend parameter. This hooks up Buddy middleware (tying the -parse and -authenticate members together, ensuring that if -parse return nil then -authenticate isn\u0026rsquo;t called)\nThe Implementation The custom backend is created by the jws-embedded-backend function, shown below. As you can see, it implements the two required protocols.\nThe backend is implemented in such a way as to authenticate using either the session or the header information. This is accomplished in the -parse function which calls parse-embedded-header. If a request session is detected and it contains an :identity field then parse-embedded-header simply returns a 2-vector of `[:session ].\nIf not, and the request headers contain a field matching one of the names we\u0026rsquo;ve reserved for our JWT token the parse-embedded-header returns a 2-vector of [:token \u0026lt;embedded-token-value\u0026gt;].\nBy implementing the backend in this way, and in the abscence of embedded headers, the session-based authentication logic will work as before when we used Buddy\u0026rsquo;s included session backend, and any authentication logic based on the request\u0026rsquo;s headers is skipped.\n(defn jws-embedded-backend\r[{:keys [authfn unauthorized-handler options token-names on-error]}]\r{:pre [(ifn? authfn)]}\r(reify\rproto/IAuthentication\r(-parse [_ request]\r(parse-embedded-header request token-names))\r(-authenticate [_ request [auth-type session-or-token]]\r(try\r;; parse-embedded-header will return a 2-vector\r;; with either :session of :token in the first\r;; position indicating how the identity is/should be\r;; determined.\r(case auth-type\r:token\r;; if we've a token, attempt to identify the user using it\r(authfn request session-or-token)\r:session\r;; if we've a session with an identity, just accept it\rsession-or-token)\r(catch clojure.lang.ExceptionInfo e\r(let [data (ex-data e)]\r(when (fn? on-error)\r(on-error request data))\rnil))))\rproto/IAuthorization\r(-handle-unauthorized [_ request metadata]\r(if unauthorized-handler\r(unauthorized-handler request metadata)\r(#'token/handle-unauthorized-default request)))))\r If the -parse function returns a non-nil value then -authenticate is called and the case statement will decide how to handle the actual authentication. As with the session backend if the 2-vector\u0026rsquo;s first element is :session we use the second element of the vector as the identity. On the other hand, if the first element is :token we call the authfn to validate the token and extract the user\u0026rsquo;s identity.\nThe implementation of -handle-unauthorized either calls the default unauthorized handler, or a handler function passed to the jws-embedded-backend function using the :unauthorized-handler key in the options map.\nAlso, authfn is an option parameter passed to the jws-embedded-backend function and is created when we def\u0026rsquo;d the backend\n(def alloc-auth-google-header-token-auth-backend\r(jws-embedded\r{:authfn token-authfn\r:token-names [\u0026quot;x-goog-iap-jwt-assertion\u0026quot; \u0026quot;x-debug-token\u0026quot;]\r:on-error\r(fn [request ex-data]\r(log/error\r\u0026quot;Request to\u0026quot;\r(:uri request)\r\u0026quot;threw exception: \u0026quot;\rex-data))\r:unauthorized-handler\r(fn unauthorized-handler\r[request metadata]\r(let [{user :user\rroles :roles\rrequired :required\ruser-session :user-session}\r(get-in metadata [:details :request])\ruser-response-session\r(keyword\r(if user (name user) \u0026quot;anonymous\u0026quot;)\r(if user-session (name user-session) \u0026quot;anonymous\u0026quot;))\rerror-message\r(str \u0026quot;NOT AUTHORIZED (HEADER): In unauthenticated handler for \u0026quot;\r\u0026quot;uri: \u0026quot; (pr-str (:uri request)) \u0026quot;. \u0026quot;\r\u0026quot;user: \u0026quot; (pr-str user) \u0026quot;, \u0026quot;\r\u0026quot;roles: \u0026quot; (pr-str roles) \u0026quot;, \u0026quot;\r\u0026quot;required roles: \u0026quot; (pr-str required) \u0026quot;. \u0026quot;\r\u0026quot;session-id: \u0026quot; (pr-str user-response-session) \u0026quot;.\u0026quot;)]\r(if user-session\r(rlog/with-forward-context\ruser-response-session\r(log/error\rerror-message)\r{:message-type :error})\r(log/error\rerror-message)))\r(cond\r;; If request is authenticated, raise 403 instead\r;; of 401 (because user is authenticated but permission\r;; denied is raised).\r(auth/authenticated? request)\r(-\u0026gt; (ring-response/response\r{:reason\r(str \u0026quot;Header authenticated, but not authorized for access to \u0026quot;\r(some-\u0026gt; metadata :details :request :path) \u0026quot;. \u0026quot;\r\u0026quot;Metadata : \u0026quot; (pr-str metadata))})\r(assoc :status 403))\r;; In other cases, respond with a 401.\r:else\r(let [current-url (:uri request)]\r(-\u0026gt;\r(ring-response/response\r{:reason \u0026quot;Unauthorized\u0026quot;})\r(assoc :status 401)\r(ring-response/header \u0026quot;WWW-Authenticate\u0026quot; \u0026quot;tg-auth, type=1\u0026quot;)))))}))\r The :token-names entry is a vector (in priority order) of the request headers we will inspect to find the user\u0026rsquo;s JWT identity token. In the code above, if the request header x-goog-iap-jwt-assertion is found then its value will be used, otherwise x-debug-token will be used. If neither is found, nil is returned. The vector\u0026rsquo;s ordering means that a x-goog-iap-jwt-assertion header injected by Google IAP will take precedence.\nIn either case the token extracted will be validated by the function passed in the option maps :authfn field (i.e. token-authfn).\n(defn token-authfn\r[request token]\r(log/info \u0026quot;Attempting to authorize using token with JWT header: \u0026quot;\r(some-\u0026gt; token (jws/decode-header)))\r(when token\r(let\r[validity-map\r(if (= \u0026quot;local\u0026quot; (:kid (jws/decode-header token)))\r{:aud \u0026quot;local\u0026quot;\r:iss \u0026quot;local\u0026quot;}\r{:aud server-config/google-jwt-audience\r:iss \u0026quot;https://cloud.google.com/iap\u0026quot;})]\r(try\r(when-let\r[user-id\r(google/get-valid-user-id-from-header\rtoken\rvalidity-map\r240)]\r(log/info \u0026quot;Successfully retrieved user ID using token \u0026quot; user-id)\ruser-id)\r(catch clojure.lang.ExceptionInfo e\r(log/info\r(str \u0026quot;Exception thrown while retrieving user from token. Message: \u0026quot;\r(.getMessage ^Throwable e) \u0026quot;, Data: \u0026quot; (pr-str (ex-data e))))\r(throw e))))))\r Here, we use the get-valid-user-id-from-header function to validate the token and extract from it the user\u0026rsquo;s id. We pass to it the token, a validity-map, indicating what the :iss and :aud value are expected to be, and a jitter value (as a number of minutes).\nThe validity-map is a map, based on the :kid value of the signed token, containing the acceptable values for the issuer and the audience fields of the token for a particular :kid. The jitter value is the number of minutes after the token\u0026rsquo;s actual expiration date (its :exp field) that the token is still considered valid. Because, this is a demonstration project we have set the value to 4 hours (240 minutes).\nIf the JWT token is valid then get-valid-user-id-from-header returns an identity-token as explained here. The :alloc-auth/token key will contain the user\u0026rsquo;s JWT token, whether :local or :google.\nThe use of a :local token is convenient for debugging purposes when you\u0026rsquo;re running the code locally, and not behind an IAP. A local token can be used by supplying it in the x-debug-token request header, and it will be used if an IAP header doesn\u0026rsquo;t exist. (You will recall that the x-goog-iap-jwt-assertion header has precedence).\nA local token can be created using the same underlying method for the :ext-token field of the identity-token. It is a JWT token created and signed by our Pedestal server.\nFor convenience, the server supplies a route to retrieve a :local token and\n$ curl --insecure -X POST https://\u0026lt;server\u0026gt;:\u0026lt;port\u0026gt;/debug/token/\u0026lt;email-address\u0026gt;\r will return a clojure map that contains a token associated with the email address you supplied in the URL. Subsequent requests can use the response\u0026rsquo;s :token field as the value of the x-debug-token request header.\nObviously, this route, which allows public access, should be disabled in a production server.\n(defn get-valid-user-id-from-header\r[google-header-assertion valid-map \u0026amp; [minutes-offset]]\r(letfn\r[(get-user-id-with-validation\r[r-val v-map]\r(log/info \u0026quot;Checking token values for validity:\u0026quot; r-val\r\u0026quot;against\u0026quot; v-map)\r(when\r(every?\r(fn [kw]\r(= (get r-val kw) (get v-map kw)))\r(keys v-map))\r(log/info \u0026quot;token is valid, email address is\u0026quot; (get r-val :email))\r{:alloc-auth/user-id\r(auth-utils/get-id-from-email-address\r(get r-val :email))\r:alloc-auth/token-type :google\r:alloc-auth/token r-val\r:alloc-auth/ext-token (debug-sign/sign-using-debug-key\rserver-config/debug-local-jwt\r{:email (get r-val :email)})}))]\r(let\r[decoded-header\r(try\r(jws/decode-header google-header-assertion)\r(catch clojure.lang.ExceptionInfo e\r(throw\r(ex-info \u0026quot;Unable to decode assertion header as jws\u0026quot;\r(merge\r(ex-data e)\r{:ex-message (.getMessage ^Throwable e)\r:ex-cause (.getCause ^Throwable e)})\re)))\r(catch Exception e\r(throw\r(ex-info \u0026quot;Unable to decode assertion header as jws\u0026quot;\r{:ex-message (.getMessage ^Throwable e)\r:ex-cause (.getCause ^Throwable e)}\re))))]\r(log/info \u0026quot;Successfully decoded header assertion using :kid\u0026quot; (:kid decoded-header))\r(log/info \u0026quot;Cache agent contains\u0026quot;\r(if-let\r[jwks (some-\u0026gt; (deref jwk-cache-agent) :jwk)]\r(str (count jwks) \u0026quot; entries with kid's \u0026quot; (mapv :kid jwks))\r\u0026quot;no entries.\u0026quot;))\r(if-let\r[public-key-jwt\r(some\r(fn [poss]\r(when\r(= (:kid poss)\r(:kid decoded-header))\rposs))\r(get (deref jwk-cache-agent) :jwk))]\r(let\r[alg (keyword (str/lower-case (:alg public-key-jwt)))]\r(try\r(-\u0026gt;\r(jwt/unsign\rgoogle-header-assertion\r(keys/jwk-\u0026gt;public-key public-key-jwt)\r{:alg alg\r:now (time/minus (time/instant) (time/minutes (or minutes-offset 0)))})\r(get-user-id-with-validation valid-map))\r(catch Exception e\r(log/error\r\u0026quot;Google header validation problem, \u0026quot;\r(pr-str (ex-data e))\r\u0026quot;using public key\u0026quot;\r(pr-str public-key-jwt)))))\r(do\r(log/error \u0026quot;No matching :kid entry found in cache.\u0026quot;)\r(throw\r(let [e (Exception.\r(str \u0026quot;No matching :kid (\u0026quot;\r(pr-str decoded-header)\r\u0026quot;) entry found in cache.\u0026quot;))]\r(ex-info \u0026quot;Bad jws.\u0026quot;\r(merge\r(ex-data e)\r{:ex-message (.getMessage ^Throwable e)\r:ex-cause (.getCause ^Throwable e)})\re))))))))\r Testing the Implementation Now that we have the backend working, we can test it using curl.\n$ curl --insecure -X POST https://localhost:8081/api/getsecresource/a\r will return the transit response\n[\u0026quot;^ \u0026quot;,\u0026quot;~:reason\u0026quot;,\u0026quot;Unauthorized\u0026quot;]\r indicating that the request couldn\u0026rsquo;t be authenticated using either a session of a header token. This is as we\u0026rsquo;d expect.\nNow issue a request for a token by supplying an email address associated with the user-id :user\n$ curl --insecure -X POST https://localhost:8081/debug/token/user@timpsongray.com\r which will return something similar to\n{:identity {:alloc-auth/user-id :user, :alloc-auth/token-type :local, :alloc-auth/token {:email \u0026quot;user@timpsongray.com\u0026quot;, :iss \u0026quot;local\u0026quot;, :aud \u0026quot;local\u0026quot;, :iat 1593088750, :exp 1593089650}, :alloc-auth/ext-token \u0026quot;eyJhbGciOiJFUzI1NiIsImtpZCI6ImxvY2FsIn0.eyJlbWFpbCI6InVzZXJAdGltcHNvbmdyYXkuY29tIiwiaXNzIjoibG9jYWwiLCJpYXQiOjE1OTMwODg3NTAsImF1ZCI6ImxvY2FsIiwiZXhwIjoxNTkzMDkyMzUwfQ.oqGnFqQ0N-7OYVBpLsiF_rNkDCtuHvI8i7OfXHaKvtJsTxxtoPHXN1kCd9d5X9Li3PM7q1Xg0z7TuRoZuGXB5A\u0026quot;, :alloc-auth/user-session :user-3}, :token \u0026quot;eyJhbGciOiJFUzI1NiIsImtpZCI6ImxvY2FsIn0.eyJlbWFpbCI6InVzZXJAdGltcHNvbmdyYXkuY29tIiwiaXNzIjoibG9jYWwiLCJpYXQiOjE1OTMwODg3NTAsImF1ZCI6ImxvY2FsIiwiZXhwIjoxNTkzMDkyMzUwfQ.tX6bAIqK7tPEpilwTtN-VWoNDgIJZXy7cxWgWmlKyfZi5Wt4R_emuBHPD6J6-WevwblA1V_f1pkWBKxH7QyFQg\u0026quot;}\r The token is in the :token field.\nUsing this token we can now issue a request for a resource to which :user is permitted access, adding the token we recieved as the x-debug-token request header,\n$ curl --insecure -X POST \\\r-H \u0026quot;x-debug-token: eyJhbGciOiJFUzI1NiIsImtpZCI6ImxvY2FsIn0.eyJlbWFpbCI6InVzZXJAdGltcHNvbmdyYXkuY29tIiwiaXNzIjoibG9jYWwiLCJpYXQiOjE1OTMwODg3NTAsImF1ZCI6ImxvY2FsIiwiZXhwIjoxNTkzMDkyMzUwfQ.tX6bAIqK7tPEpilwTtN-VWoNDgIJZXy7cxWgWmlKyfZi5Wt4R_emuBHPD6J6-WevwblA1V_f1pkWBKxH7QyFQg\u0026quot; \\\rhttps://localhost:8081/api/getsecresource/u\r and we should see the following transit response,\n[\u0026quot;^ \u0026quot;,\u0026quot;~:the-results\u0026quot;,\u0026quot;Let's pretend that this is something interesting.\u0026quot;]\r letting us know that the request succeeded.\nHowever, if we try to access a resource to which :user does not have permission,\n$ curl --insecure -X POST \\\r-H \u0026quot;X-debug-token: eyJhbGciOiJFUzI1NiIsImtpZCI6ImxvY2FsIn0.eyJlbWFpbCI6InVzZXJAdGltcHNvbmdyYXkuY29tIiwiaXNzIjoibG9jYWwiLCJpYXQiOjE1OTMwODg3NTAsImF1ZCI6ImxvY2FsIiwiZXhwIjoxNTkzMDkyMzUwfQ.tX6bAIqK7tPEpilwTtN-VWoNDgIJZXy7cxWgWmlKyfZi5Wt4R_emuBHPD6J6-WevwblA1V_f1pkWBKxH7QyFQg\u0026quot; \\\rhttps://localhost:8081/api/getsecresource/a\r we\u0026rsquo;ll receive an error message letting us know that :admin is the required role, while we only have the :user role,\n[\u0026quot;^ \u0026quot;,\u0026quot;~:reason\u0026quot;,\u0026quot;Header authenticated, but not authorized for access to /api/getsecresource/a. Metadata : {:details {:request {:path \\\u0026quot;/api/getsecresource/a\\\u0026quot;, :path-params {}, :user :user, :roles #{:user}, :identity #object[clojure.core$identity 0x8a41b2d \\\u0026quot;clojure.core$identity@8a41b2d\\\u0026quot;], :required #{:admin}, :user-session nil}, :message \\\u0026quot;\\\\\\\u0026quot;Alloc-Unauthorized\\\\\\\u0026quot;\\\u0026quot;}}\u0026quot;]\r Closure We now have a authentication back-end that can handle both session- and header-based authentication. Our SPA works as it always did, but now we can also use the api from non-browser based clients, including requests received through, and tagged by the IAP proxy.\n","date":1591436984,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593110503,"objectID":"3d14393f8514b88af71f6f2a40595a44","permalink":"https://heykieran.github.io/post/adding-a-custom-auth-backend/","publishdate":"2020-06-06T05:49:44-04:00","relpermalink":"/post/adding-a-custom-auth-backend/","section":"post","summary":"I discuss the available backends available in the buddy-auth library and how they are implemented. I will then show, for circumstances where the provided back-ends don't meet our needs how we can implement our own.","tags":["clojure","pedestal","buddy-auth","clojurescript","reframe","reagent"],"title":"Add a Custom Authentication Backend to a Clojure Pedestal Application using Buddy","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction In this piece I show how to integrate Buddy\u0026rsquo;s authentication and authorization functionality with a Pedestal web application.\nI provide a brief overview of Pedestal\u0026rsquo;s interceptor model, particularly error-handling, which we\u0026rsquo;ll use to catch and handle any authentication or authorization errors thrown by Buddy.\nI also cover some of Buddy\u0026rsquo;s available functionality; how to decide if access to a web resource should be allowed; and how to respond to the requesting client when access is denied.\nWhat\u0026rsquo;s not covered? For the purpose of this discussion, there is an assumption that the application is using session-based security. However, there is only a short discussion of how the user session is actually populated for use by Pedestal and Buddy.\nThis is an important topic and if you want to learn more about how to manage the login and session population processes, you can review the following blog post, where I\u0026rsquo;ve covered it more fully, or the source code for the application, using session based authentication, is available on GitHub (tag v1.0).\nThe Web Application The web application will proceed through the following steps:\n The user requests a resource located at a URL on the server The application identifies \u0026amp; authenticates the user The application determines the valid role(s) for the user The application determines if the user\u0026rsquo;s role(s) permit access to the resource The application either serves the resource or returns an access-denied response.  Initially, I elide certain details, such as how the user asserts an identity, but return to them later in the piece.\nPedestal Pedestal is a set of libraries developed by the team at Cognitect to facilitate the creation of web applications in Clojure. It uses the interceptor pattern, which differs from the handler/middleware pattern adopted by Ring.\nOversimplifed, but adequate for our purposes, a Pedestal application is a chain of interceptors with each interceptor being an entity with an :enter function, a :leave function, and an :error function. The web-server receives a request, bundles it into a context (a map) and then threads that context through each interceptor, which has an opportunity to change it, producing a final context which is marshalled into an HTTP response and returned to the requesting client.\nThe chaining logic through the interceptors is two-pass.\nIf we consider a chain of interceptors I1, I2 and I3: Pedestal will push the context through I1, then I2 and then I3. It will then pull the context back through the chain in reverse order i.e. I3, then I2, and finally I1. In the push phase each interceptor\u0026rsquo;s :enter function will be called with the context as its argument, and the return value (also a context) will be passed to the next interceptor\u0026rsquo;s :enter function. During the pull phase, each interceptor\u0026rsquo;s :leave function will be called. As with the push phase, each interceptor\u0026rsquo;s :leave function will receive the context as its argument and is expected to return a context which is passed to the next interceptor.\n\rIn the discussion above, I\u0026rsquo;ve stated that an interceptor function receives a context and returns a context. This is conceptually correct, but obscures one of Pedestal\u0026rsquo;s useful features.\nAn interceptor function can also return a core.async channel. When this happens, Pedestal will yield the thread, allowing other activity to occur, and will recommence when a value is available on the channel. When that happens, Pedestal will read one value from the channel (which must be a context) and continue processing the chain.\n\r\rAlthough it\u0026rsquo;s not particularly relevant to our current discussion, it\u0026rsquo;s important to note that the interceptor chain is not fixed in any general sense. An interceptor, within the context it receives, has access to the chain itself and can manipulate it.\nWhat is relevant is how the Pedestal interceptor chain handles errors and exceptions. Although the Pedestal interceptor call sequence is analagous to a function call stack, the calls do not exist in that nested manner on the JVM call stack. Therefore, it\u0026rsquo;s not possible to use Clojure\u0026rsquo;s try/catch mechanism to catch exceptions thrown by one interceptor in a different interceptor further up the stack.\nThe Pedestal machinery catches any exception thrown by an interceptor, wraps it in an ExceptionInfo instance and associates the instance into the context map with the key :io.pedestal.interceptor.chain/error. Pedestal then back tracks through the interceptor chain looking for an interceptor with an :error function that can handle the exception.\nEach :error function is called with two arguments, the context (without the :io.pedestal.interceptor.chain/error key) and the ExceptionInfo instance. If an interceptor\u0026rsquo;s :error function can handle the error the interceptor should return a context. In that case, Pedestal will continue processing the remaining interceptor\u0026rsquo;s :leave functions and ultimately return a response to the client.\nIf an interceptor\u0026rsquo;s :error function cannot handle the exception it should reattach the ExceptionInfo instance it received to the context (as :io.pedestal.interceptor.chain/error) and return the new updated context. This allows Pedestal to continue searching for an appropriate handler.\nDuring Pedestal\u0026rsquo;s exception handling process no :enter or :leave functions will be called while the context map contains a :io.pedestal.interceptor.chain/error entry.\nBuddy \rBuddy is a Clojure library providing authentication and authorization facilites to web applications. Although its documentation is primarily focused on ring based applications, the library itself is flexible enough to be used with just about any Clojure web-application library including Pedestal.\nBuddy treats authentication and authorization as independent concerns. Authentication decides who you are, and authorization determines what you can do. I discuss authentication first, and return to the topic of authorization.\nWithin Buddy there are available many mechanisms to authenticate the user, and Buddy refers to these machanisms as backends. The two we will discuss in greater detail below are session and (in a different blog post) jws (signed JWT).\nConceptually, for all backends, Buddy\u0026rsquo;s authentication functionality is extremely simple. It occurs in two phases: a parse phase, and an auth phase\nParse Phase During the parse phase the backend takes the http request (for example, contained in Pedestal\u0026rsquo;s context map) and extracts from it any values required by the backend\u0026rsquo;s auth phase. These values could be in the request\u0026rsquo;s headers, session, query params etc; it depends on the backend. If the parsing of the request returns nothing (nil/false) then further processing by Buddy stops and the auth phase is not entered - the request (and the requestor) is considered unauthenticated. Otherwise, the relevant values from parsing are passed to the auth phase.\nAuth Phase During the auth phase, the values returned by the parse phase are used to determine the identity of the user. This involves calling the backend\u0026rsquo;s authentication function (auth-fn) with those values. If the auth-fn returns a non-nil, non-false value then the request map\u0026rsquo;s :identity key is set to that value. This value represents an authenticated user. As with parsing, how authentication is done depends on the backend in use. Possibly, it\u0026rsquo;s an extraction of a session identifier followed by a database lookup, or even a decryption and verification of a signed JWT that was parsed from the request\u0026rsquo;s headers.\nBuddy provides a number of backend, but you\u0026rsquo;re also free to define your own if they do not meet your needs.\nReview of Application Considering what we know about interceptors and Buddy, we can now sketch out an approach to securing the application, and then its implementation using Pedestal interceptors.\nThe Security Model User The basic entity is the user. A user can be associated with one or more email addresses. At a later stage this will allow the application use a variety of external identity providers, such as Google, Facebook, Azure etc. Obviously, an email address can be associated with only one application user entity.\nRole A user can also be assigned to one or more roles. Roles determine a user\u0026rsquo;s permissions within the web application. Roles exist independently of each other and within a flat structure. There are no concepts of hierarchy and inheritance.\nRoute Roles are granted (or denied) HTTP verb access to individual uri\u0026rsquo;s which are represented in the Pedestal world as routes. A route may also be unprotected meaning that its uri is accessible to unauthenticated users (i.e. the public).\nThere are two ways to allow unprotected access to a resource\n  Do not use interceptors that manage or restrict access based on identity and permissions. (These interceptors are established for a route with the build-secured-route-vec-to function, which is dicussed in greater detail below).\n  Name the route using a value in the :alloc-public namespace (see note below on how the permissions lookup table is populated).\n  Example The following code fragment defines three routes\n(def routes\r(route/expand-routes\r#{[\u0026quot;/\u0026quot; :get landing-page :route-name :landing-page]\r[\u0026quot;/api/htest\u0026quot; :get (build-secured-route-vec-to test-response) :route-name :alloc-user/auth-test-response-get]\r[\u0026quot;/api/htest\u0026quot; :post (build-secured-route-vec-to test-response)\r:route-name :alloc-admin/auth-test-response-post]}))\r Each vector in the set passed to expand-route, contains a uri pattern, a method, an interceptor (or vector of interceptors) and a :route-name key with its associated value. The application will use the namespace of the route name to build a permission table linking a uri to a role.\nIn the fragment above the path / is available to any user (including unauthenticated users) as the only active interceptor is the landing-page interceptor, which uses no authentication.\nThe ability to GET from /api/htest is restricted to users with the :user role, and the ability to POST to the uri is restricted to users with the :admin role. This is ensured because the build-secured-route-vec-to function inserts the necessary authentication and permission checking interceptors into the uri\u0026rsquo;s interceptor chain before the test-response handler/interceptor.\nThis approach is helpful as Pedestal, when seeing a pure handler function as the last entry in an interceptor vector, will convert it to an interceptor. (A pure handler function is a single arity function taking a request as its argument). This means that a handler function can be fully exercised in the REPL before attempting to secure it.\nUsing the information encoded in the routes, the permission table will be constructed by the application at runtime and is used by the interceptor responsible for checking permissions that the authenticated user is in a role required to access the resource.\n The Log In Process In general, and elliding how claims are actually made, when the user attempts to log in the following sequence of events occurs on the server,\n  The user asserts a claim that they are a valid user with a particular email address\n  If the server finds this claim to be valid, it will create an identity-token map with the following keys\n   Name Explanation     :alloc-auth/user-id The user\u0026rsquo;s id e.g. :admin or :act-user   :alloc-auth/token-type The assertion type made by the user (local or google) and verified by the server   :alloc-auth/token The user\u0026rsquo;s token. This will contain fields :email, :iss, :aud, :iat and :exp, which are the user\u0026rsquo;s email address, the token\u0026rsquo;s issuer, an audience indicator, the time of the token\u0026rsquo;s issuance, and the time of its expiration.   :alloc-auth/ext-token A jwt token created and signed by the server. It contains the user\u0026rsquo;s email address and can be used by the client to assert an identity independently of the Pedestal interceptor chain. This may be necessary when a session isn\u0026rsquo;t available i.e. a client can make a request to an api endpoint by including this token in the request\u0026rsquo;s headers, or when a request to an endpoint outside the security context of Pedestal\u0026rsquo;s interceptor chain is made e.g. a websocket connection, which is made at the Jetty Session level.   :alloc-auth/user-session A keyword that is a combination of the user\u0026rsquo;s id and a monotonically increasing sequence number (e.g. :admin-8 for user :admin). Each Pedestal (Ring) session will be associated with a unique user-session.      The server then adds the identity-token to its record of logged in users, stored in the server.auth.data/alloc-auth-logged-in-users atom.\nThis atom retains a list of all the currently logged in users. It is a map where the key is a vector combining the user-id the user-session and a text flag, that is currently always the string \u0026ldquo;single-session-only\u0026rdquo;. The value stored in this map is the identity-token.\n  The server will also write the identity-token to the Ring session\u0026rsquo;s :identity field\n  The server will send the Ring session\u0026rsquo;s identifier as a secure http-only cookie in a transit response to the client.\n  This completes the initial client/server login and authentication process.\nThe Interceptor Chain Now, a consideration of the interceptor chain built by the build-secured-route-vec-to function. This function taking as its first argument an interceptor (or handler function) will return a vector of interceptors appropriate for the dual functions of authenticating the user and authorizing his/her access to the resource (uri). The function also accepts a number of other options, which we will return to later.\n  The first interceptor in the security-related portion of the interceptor chain attempts to authenticate the user. It is provided a context map, and will update the :request portion of the context map using the buddy.auth.middleware/authentication-request function. This function takes as parameters the request, and the backend. It will populate the :identity key of the request map if authentication suceeds (as determined by the backend). If the authentication fails the backend\u0026rsquo;s :unauthorized-handler is called. This returns a 401 response to the client.\n  The next interceptor retrieves the :identity from the the context\u0026rsquo;s :request map and looks up the roles associated with the user. It attaches the information retrieved to the context map using the key :alloc-auth/auth. The value added will be a map with two keys :user and :roles. By attaching this information to the context map, it becomes available for interceptors later in the chain.\n  Then the error catching interceptor (created by the function alloc-auth-unauthorized-interceptor) is entered. It returns the received context map unchanged. It\u0026rsquo;s only responsibility is to handle exceptions that might be thrown later by the interceptors named :alloc-auth-permission-checker and :alloc-auth-access-rule-checker. These two interceptors in turn check the user\u0026rsquo;s access to a resource (uri) based on his/her assigned roles; and checks his/her access based on custom defined rules. (I provide a expanded description of both below.)\n\rA benefit of using a single error catching interceptor is that there is a consolidation of application responses in a single area of the code rather than having them spread throughout the code in other interceptors' :error functions. The interceptor is created by the alloc-auth-unauthorized-interceptor function which will select the appropriate backend at the time an authentication or authorization error is encountered. Therefore, in addition to the consistency of responses mentioned above, this approach ensures that backends are fully swappable and can be changed without impacting other areas of the code.\r\r  \r  The next interceptor in the chain (:alloc-auth-permission-checker) extracts the :alloc-auth/auth value from the context map and compares its :roles value against the roles required to access the resource using the permissions table. If the comparison fails to find a match between the user\u0026rsquo;s assigned roles and the roles required to access the resource, the interceptor will throw an \u0026ldquo;Alloc-Unauthorized\u0026rdquo; exception. This will cause Pedestal to start looking for a handler, finding it in the error catching interceptor (see above), which ultimately returns a 401 or 403 response to the client.\n  If the request has gotten this far, then the user is superficially allowed to access the resource (by HTTP verb and uri), but there may be other, finer restrictions to be considered. The final interceptor related to security is now entered (:alloc-auth-access-rule-checker). It takes the context map\u0026rsquo;s :request value and runs a set of rules against it using functions in the buddy.auth.accessrules namespace. If these rules result in a success the resource is returned to the client; if not, an exception is thrown, which is again handled by the error catching interceptor. This will return a 401 or 403 response to the client as appropriate.\n  Access Rules Access rules are helpful when an application developer want to allow access to a route only under certain circumstances; circumstances that cannot be encoded in a route\u0026rsquo;s uri pattern. As a trivial example, consider the situation where the developer wants to grant access to a uri pattern /api/dostuff/:id between 9:00AM and 5:00PM only.\nA way to achieve this is to use an access rule defined according to the convention required by buddy.auth.accessrules.\nSuch a rule can be expressed as follows\n(def rule-1\r[{:uri \u0026quot;/api/dostuff/:id\u0026quot;\r:handler\r(fn [request]\r(let\r[d (time/local-date) n (time/local-date-time)]\r(if (time/before?\r(time/local-date-time (str d \u0026quot;T09:00:00\u0026quot;))\rn\r(time/local-date-time (str d \u0026quot;T17:00:00\u0026quot;)))\r(buddy.auth.accessrules/success)\r(buddy.auth.accessrules/error))))}])\r If it was required that the role should be granted access during those hours only, and when the :id parameter is equal to \u0026ldquo;company1\u0026rdquo; the rule would be\n(def rule1\r[{:uri \u0026quot;/api/dostuff/:id\u0026quot;\r:handler\r(fn [request]\r(let\r[{company-id :id}\r(-\u0026gt; request :match-params)\ruser-identity (-\u0026gt; request :identity)\rauth? (buddy.auth/authenticated? request)\ruri (-\u0026gt; request :uri)\rd (time/local-date)\rn (time/local-date-time)]\r(if (and\r(= company-id \u0026quot;company1\u0026quot;)\r(time/before?\r(time/local-date-time (str d \u0026quot;T09:00:00\u0026quot;))\rn\r(time/local-date-time (str d \u0026quot;T17:00:00\u0026quot;))))\r(buddy.auth.accessrules/success)\r(buddy.auth.accessrules/error))))}])\r  Because, buddy-auth attaches the user\u0026rsquo;s identity to the request map in the context map, it can be retrived and used during the processing of an access rule. Also, any path params extracted from the uri will be available in the handler function in the request map\u0026rsquo;s :path-params field.\n When a rule returns error it results in an unauthorized exception being raised by the backend. This exception is caught in the error catching interceptor as before.\nBuddy Backends As previously mentioned, a backend is responsible for providing a function (authfn) that can authenticate a user, a function (unauthorized-handler) responsible for handling authentication and authorization failures, and possibly a function (on-error) to handle errors.\nInternally, a backend is an instance of an object that implements two protocols defined in the buddy.auth.protocols namespace, namely IAuthentication and IAuthorization.\nThe IAuthentication protocol must provide the -parse method, a function to extract any required information from the suplied request; and the -authenticate method, a function to authenticate the user. The -authenticate method will call the authfn function passed when the backend is created in the application.\nThe IAuthorization protocol must provide the -handle-unauthorized method which will call the unauthorized-handler function with the request map and a metadata argument describing the failure.\nFortunately, buddy-auth comes with a number of built-in backends.\nThe Session Back-End One of the back-ends provided by Buddy is session, which relies on ring\u0026rsquo;s session support. During the parse phase, the request\u0026rsquo;s session map is inspected for an :identity key. If that key exists it\u0026rsquo;s passed to the auth phase, which simply sets the request map\u0026rsquo;s :identity key to that value. It\u0026rsquo;s really that simple.\nIf you use sessions there are a number of security implications that you should consider. First, although the complete session information exists only on the server, the session\u0026rsquo;s identifier is passed back and forth between the client and the server, and despite some of the security mechanisms employed by browsers (and user agents, more generally), and the cookie-based session functionality provided by ring you will need to be careful.\nYou should only use https. This ensures that the information passed between the client and the server is encryped in transit. Also, cookies should be set to Secure. Also, you should consider strongly the use of HttpOnly and SameSite. OWASP provides some very good information regarding session security, and you should review it.\nUsing Ring Session Middleware with Pedestal Because of the fundamental differences between Pedestal\u0026rsquo;s interceptor model and Ring\u0026rsquo;s wrapped middleware model, Pedestal provides in its io.pedestal.http.ring-middlewares namespace an ability to adapt a Ring middleware function to an interceptor context. Conveniently, the namespace also provides a function (session) which does this specifically for adapting Ring\u0026rsquo;s session middleware. We only need to include the interceptor returned by this function in our interceptor chain to make use of Ring sessions in our Pedestal application.\nUsing Buddy\u0026rsquo;s session back-end with Pedestal Buddy provides an implementation of the session back-end in the buddy.auth.backends namespace and it can be instantiated using the buddy.auth.backends/session function. This function can also accept an options map containing :authfn and :unauthorized-handler keys, which if supplied are expected to be functions that handle authentication and what to do when a request is not authorized respectively. If neither is supplied, Buddy will supply sensible defaults.\nFor our purposes, the default :authfn function will suffice, but because we will later have to handle authorization we will provide our own :unauthorized-handler function.\n(def alloc-auth-session-auth-backend\r(auth.backends/session\r{:unauthorized-handler\r(fn unauthorized-handler\r[request metadata]\r(let [{user :user roles :roles required :required\ruser-session :user-session}\r(get-in metadata [:details :request])\rerror-message\r(str \u0026quot;NOT AUTHORIZED (SESSION): In unauthenticated handler for \u0026quot;\r\u0026quot;uri: \u0026quot; (pr-str (:uri request)) \u0026quot;, \u0026quot;\r\u0026quot;and path-params \u0026quot;\r(pr-str (:path-params request)) \u0026quot;. \u0026quot;\r\u0026quot;user: \u0026quot; (pr-str user) \u0026quot;, \u0026quot;\r\u0026quot; roles: \u0026quot; (pr-str roles) \u0026quot;. \u0026quot;\r\u0026quot;required: \u0026quot; (pr-str required) \u0026quot;. \u0026quot;\r\u0026quot;user-session: \u0026quot; (pr-str user-session) \u0026quot;.\u0026quot;)]\r(if user-session\r(rlog/with-forward-context\ruser-session\r(log/error\rerror-message))\r(log/error\rerror-message)))\r(cond\r;; If request is authenticated, raise 403 instead\r;; of 401 (because user is authenticated but permission\r;; denied is raised).\r(auth/authenticated? request)\r(-\u0026gt; (ring-response/response\r{:reason\r(str \u0026quot;Authenticated, but not authorized for access to .\\n\u0026quot;\r\u0026quot;Metadata : \u0026quot; (pr-str metadata))})\r(assoc :status 403))\r;; In other cases, respond with a 401.\r:else\r(let [current-url (:uri request)]\r(-\u0026gt;\r(ring-response/response\r{:reason \u0026quot;Unauthorized\u0026quot;})\r(assoc :status 401)\r(ring-response/header \u0026quot;WWW-Authenticate\u0026quot; \u0026quot;tg-auth, type=1\u0026quot;)))))}))\r The Route and Interceptor Implementations Building the Interceptor Chain For each Pedestal route defined in the application, an interceptor chain (a vector of interceptors) is constructed and included in the service map which is passed to io.pedestal.http/start to start the server. The application uses a function build-secured-route-vec-to to return a vector of interceptors that are installed for the route. The vector returned will include a number of common interceptors in addition to the security-related interceptors we\u0026rsquo;ve been discussing.\nThe build-secured-route-vec-to function takes as parameters a handler (or interceptor), and potentially two option parameters, :use-headers and :rules. The handler is installed as the last interceptor in the chain, and is expected to provide the business-logic functionality.\nIf a :rules option is supplied, it is expected to be a map conforming to the format required by buddy.auth.accessrules. The presence of the :rules option will also cause the :alloc-auth-access-rule-checker interceptor to be included in the vector of interceptors returned.\nThe :use-headers option can be ignored for now. It will be the subject of another blog post discussing how to create a custom backend for Buddy.\nThe Security Interceptors Now let\u0026rsquo;s take a closer look at the implementation details of the security-related interceptors mentioned above.\nThe :alloc-auth-authenticate interceptor This interceptor is responsible for the authentication of the user and is created by the alloc-auth-authentication-interceptor function.\n(defn alloc-auth-authentication-interceptor\r[backend]\r(interceptor/interceptor\r{:name ::alloc-auth-authenticate\r:enter (fn [ctx]\r(-\u0026gt; ctx\r(assoc\r:auth/backend\rbackend)\r(update\r:request\rauth.middleware/authentication-request\rbackend)))}))\r authentication-request is a function which takes a request and a backend and using the backend attempts to parse the request and to authenticate the user. If the user is sucessfully authenticated an :identity key is added to the request map with the value returned by the backend\u0026rsquo;s authfn function.\nThe interceptor performs two functions:\n  it attaches to the context map the authentication backend being used. This makes it available to other interceptors later in the chain, particularly the error catcher interceptor.\n  it updates and returns the context map with the (potentially) updated request map returned by the call to authentication-request\n  The :alloc-auth-user-roles interceptor This interceptor will attach to the context map information about the roles to which the user has been assigned. It is created by calling the alloc-auth-user-roles-interceptor function.\n(defn alloc-auth-user-roles-interceptor\r[]\r{:name ::alloc-auth-user-roles\r:enter (fn [ctx]\r(log/info \u0026quot;Assigning roles for identity \u0026quot; (pr-str (get-in ctx [:request :identity])))\r(let\r[{identity-user-id :alloc-auth/user-id\ridentity-token-type :alloc-auth/token-type\ridentity-token :alloc-auth/token}\r(get-in ctx [:request :identity])]\r(assoc\rctx\r:alloc-auth/auth\r{:user identity-user-id\r:roles (alloc-auth-get-roles-for-identity identity-user-id)})))})\r This interceptor was discussed quite extensively above, but two items are worth noting. The authenticated user (in the request map\u0026rsquo;s :identity field) is expected to be identified by a map with the keys :alloc-auth/user-id, :alloc-auth/token-type and :alloc-auth/token. For our current discussion the first of these is the most important, and using buddy\u0026rsquo;s session backend would have been extracted from the user\u0026rsquo;s session object. It is the internal application user id for the user e.g. :admin or :fred.\nThis value is used by alloc-auth-get-roles-for-identity to return a collection of role entities indicating with which roles the user is associated. The interceptor returns an updated context with this information attached in the :alloc-auth/auth key.\nThe Error Catcher interceptor This interceptor will catch authentication and authorization errors raised by the :alloc-auth-permission-checker and :alloc-auth-access-rule-checker interceptors (any others are ignored). This interceptor is created by calling the alloc-auth-unauthorized-interceptor function, which internally uses Pedestal\u0026rsquo;s error-dispatch function to match errors with handlers.\n(defn alloc-auth-unauthorized-interceptor\r[]\r(letfn\r[(unauthorized-fn[ctx ex]\r(if-let\r[handling-backend (:auth/backend ctx)]\r(assoc\rctx\r:response\r(.-handle-unauthorized\rhandling-backend\r(:request ctx)\r{:details\r{:request (ex-data (ex-cause ex))\r:message (pr-str (ex-message (ex-cause ex)))}}))\r(do\r(log/error \u0026quot;Unauthorized requests, but there is no backend\u0026quot;\r\u0026quot;installed to handle the exception.\u0026quot;)\r(throw \u0026quot;No auth backend found.\u0026quot;))))]\r(error-dispatch\r[ctx ex]\r[{:exception-type :clojure.lang.ExceptionInfo :interceptor ::alloc-auth-permission-checker}]\r(try\r(unauthorized-fn ctx ex)\r(catch Exception e\r(assoc ctx ::interceptor.chain/error e)))\r[{:exception-type :clojure.lang.ExceptionInfo :interceptor :alloc-auth-access-rule-checker}]\r(try\r(unauthorized-fn ctx ex)\r(catch Exception e\r(assoc ctx ::interceptor.chain/error e)))\r:else\r(assoc ctx ::interceptor.chain/error ex))))\r The function uses Pedestal\u0026rsquo;s error-dispatch function to create an interceptor that can handle ExceptionInfo exceptions thrown by either the ::alloc-auth-permission-checker or ::alloc-auth-access-rule-checker interceptors.\nIn the case of either exception, it will call the backend\u0026rsquo;s -handle-unauthorized method (from the IAuthorization protocol implemented by the backend), which ultimately calls the unauthorized-handler function registered with the backend (see the notes on alloc-auth-session-auth-backend above).\nAny errors that cannot be handled, or throw exceptions during handling are reattached to the context map - potentially to be handled by another interceptor\u0026rsquo;s :error function, or escaping at the top level with a 5xx error being returned to the client.\nNote, that the backend instance to be used when signalling an exception is retrieved from the context map. It was added to the context map by the :alloc-auth-authenticate interceptor (see above).\nThe :alloc-auth-permission-checker interceptor This interceptor checks whether the user\u0026rsquo;s roles (embedded in the context map by :alloc-auth-user-roles) allow access to the requested uri. It is created by calling the alloc-auth-permission-checker-interceptor-factory function.\n(defn alloc-auth-permission-checker-interceptor-factory\r[]\r(interceptor/interceptor\r{:name ::alloc-auth-permission-checker\r:enter (fn [ctx]\r(log/info\r(str \u0026quot;Checking Identity: \u0026quot;\r(pr-str (get-in ctx [:request :identity :alloc-auth/user-id] :unauthenticated))\r\u0026quot; for access to \u0026quot;\r(pr-str (get-in ctx [:request :path-info]))\r\u0026quot; with path params \u0026quot;\r(pr-str (get-in ctx [:request :path-params]))\r\u0026quot; for route name \u0026quot;\r(pr-str (get-in ctx [:route :route-name]))\r\u0026quot; with session \u0026quot;\r(pr-str (get-in ctx [:request :session]))))\r(let\r[{req-path :path-info\rres-path-params :path-params\r{identity-user-id :alloc-auth/user-id\ridentity-token-type :alloc-auth/token-type\ridentity-token :alloc-auth/token\ruser-session :alloc-auth/user-session} :identity}\r(get-in ctx [:request])\r{route-name :route-name route-method :method\rroute-path-re :path-re route-path-parts :path-parts\rroute-path-params :path-params}\r(get-in ctx [:route])\r{user :user roles :roles}\r(get-in ctx [:alloc-auth/auth])\rrequired-roles\r(get-in @alloc-auth-permissions [route-name :permissions :roles])]\r(log/info\r(str \u0026quot;User Roles: \u0026quot;\r(pr-str roles)\r\u0026quot; , required roles \u0026quot;\r(pr-str required-roles)))\r(if (and\r(not (contains? required-roles :public))\r(empty? (clojure.set/intersection\rroles required-roles)))\r(throw\r(ex-info \u0026quot;Alloc-Unauthorized\u0026quot;\r{:path req-path :path-params res-path-params\r:user user :roles roles :identity identity\r:required required-roles\r:user-session user-session}))\r(update-in\rctx\r[:request]\rassoc :auth-alloc \u0026quot;ok\u0026quot;))))}))\r If the roles associated with the user, and assoc-ed into the context map earlier as :alloc-auth/auth don\u0026rsquo;t intersect with the roles required for access (stored in the alloc-auth-permissions atom) an ex-info exception is thrown. The exception will be handled by the error catcher interceptor which will return the appropriate response to the client.\nThe :alloc-auth-access-rule-checker interceptor If the Pedestal interceptor chain which is built using build-secured-route-vec-to was passed a :rules parameter, this interceptor will run the rules' handler functions to decide whether access to the resource should be granted (returns success) or denied (returns error).\n(defn alloc-auth-rules-checker-interceptor-factory\r[rules]\r(interceptor/interceptor\r{:name :alloc-auth-access-rule-checker\r:enter (fn [context]\r(let\r[request (:request context)\rpolicy :allow\rw-a-rules-fn\r(auth.accessrules/wrap-access-rules\r(fn [req] :ok)\r{:rules rules :policy policy})]\r(w-a-rules-fn request)\rcontext))}))\r The final security-related interceptor uses buddy.auth.accessrules to determine if access should be granted. buddy.auth.accessrules contains a wrap-access-rules function that is helpful in Ring\u0026rsquo;s middleware context to wrap other Ring handlers. The interceptor uses this functionality by providing a synthetic handler that returns :ok. This works for our purposes, because the implementation of wrap-access-rules when called with a request will throw an exception if rules are violated for that request. This exception will be caught by the error catcher interceptor. If no exception is thrown, the interceptor returns unchanged the context map it received.\nNext Steps Hopefully, if you\u0026rsquo;ve been looking for guidance on how to integrate Buddy with Pedestal this document has helped. A later post will consider how one might provide a custom backend for Buddy.\n","date":1591188000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593110503,"objectID":"bdefc1c41e0eb3f2198207ad9b32266b","permalink":"https://heykieran.github.io/post/pedestal-buddy/","publishdate":"2020-06-03T12:40:00Z","relpermalink":"/post/pedestal-buddy/","section":"post","summary":"In this piece I will cover briefly how the Pedestal web-server operates, particularly the interceptor model and error handling; how to integrate a Pedestal web application with the buddy-auth library; and I will demonstrate with code how to secure access to Pedestal endpoints using the buddy-auth library.","tags":["clojure","pedestal","buddy","security","authentiation","authorization"],"title":"Pedestal, Buddy and Security","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction In a previous blog post I showed how it was possible to take a Clojure project containing a Pedestal back-end and a React front-end, and package it as a Docker container which can be run as a standalone Docker image using docker run, or as part of a Docker Swarm using docker service.\nIn this post, I will show how it is possible to deploy the same Docker container to a Kubernetes Cluster and to make the application available at a particular URL of your choosing.\nA repository containing the code can be found here (tag v1.2).\nSummary of the steps  Create a GCP Project Build and Tag your Docker image Upload your image to the Google Registry Create a GKE Cluster Get a Google Static IP address Create an A record in your DNS for the IP endpoint Create a Google managed SSL certificate Deploy your image to the cluster Create a back-end service over your container Create an Ingress front-end service connecting the external IP address to the back-end service over https  The Steps Setting up a Cloud Project on GCP Before you start, you\u0026rsquo;ll need to create a Google Cloud Project. This is a simple process, so I won\u0026rsquo;t go into the details as instructions can be found here.\n The current demo assumes that the project name is clojure-app-v1, that a Docker daemon is running locally, and that the docker binary is on your path. Also, the URL at which the application is published is chosen to be https://demo.timpsongray.com. Obviously, this will be different for you.\n You should also install both the gcloud and kubectl command line tools locally. If this is your first GCP project, don\u0026rsquo;t forget to also initialize the Cloud SDK. This will set your account\u0026rsquo;s credentials, authorize access to GCP API\u0026rsquo;s, and establish a base configuration, such as your default compute region and zone.\n \rInstalling gcloud \rInitializing the SDK  Once you have installed gcloud locally, installing kubectl is simply a matter of running\n$ gcloud components install kubectl\r from the command line.\nFinally, with the tools installed and your project created, you can view its details using\n$ gcloud projects describe allocations-accounting-v1\r Setting your Current Project Now, you should set allocations-accounting-v1 to be the current project. (If you\u0026rsquo;ve run gcloud init as above, this should already have been done, but it\u0026rsquo;s no harm to set it a second time.)\n$ gcloud config set project allocations-accounting-v1\r Connecting you Docker Repository to GCP In order to easily publish a docker image from your local machine to a GCP container registry you should configure docker to use gcloud as the credential helper for all Google\u0026rsquo;s registries using\n$ gcloud auth configure-docker\r Build and Tag the docker image Build the docker image If your using the repo, you can do this by issuing\n$ make clean-all\r$ make docker\r from the command line.\nTag the image for upload to the Google container registry Now we\u0026rsquo;ll tag the image so it conforms with the image names expected by the Google registry.\n$ docker tag clojure-app-v1 gcr.io/allocations-accounting-v1/allocations-accounting:v1.0\r  By default, if you don\u0026rsquo;t specify a tag the above command adds :latest to the end of the full tag name. It\u0026rsquo;s good practice to explicitly specify a tag.\n The Google registry location is constructed using gcr.io/\u0026lt;PROJECT_NAME\u0026gt;/\u0026lt;IMAGE_NAME\u0026gt;.\nPush the image to the Google Registry Before Google will accept a pushed image you need to enable the Google Container Registry API for your project\n$ gcloud services enable containerregistry.googleapis.com\r and then push the tagged image using\n$ docker push gcr.io/allocations-accounting-v1/allocations-accounting:v1.0\r Create the Cluster First enable the Kubernetes Engine API using\n$ gcloud services enable container.googleapis.com\r which may take a few minutes.\nWhen complete, we ask GKE to create a cluster with a single node, which is sufficient for illustrative purposes.\n$ gcloud container clusters create allocations-accounting-v1-cluster --num-nodes=1\r Again, this may take a few minutes as the cluster\u0026rsquo;s resources are created, deployed and health-checked.\nImport Credentials Once the cluster has been created we sync credentials\n$ gcloud container clusters get-credentials allocations-accounting-v1-cluster\r This will create a kubeconfig entry for the cluster, and allow you to manage the new cluster using the kubectl command line tool.\nGet a static IP address for your site We have decided to publish our application at a well-known url (i.e. demo.timpsongray.com), so we need to ensure that we have a stable, externally addressable IP address. We do this by asking GCP to assign a global IP address for our use.\n$ gcloud compute addresses create allocations-app-v1-addr --global\r Find the external IP address Now, find what IP address was assigned using\n$ gcloud compute addresses list\r which should return something like\nNAME ADDRESS/RANGE TYPE PURPOSE NETWORK REGION SUBNET STATUS\rallocations-app-v1-addr 34.120.154.247 EXTERNAL RESERVED\r Make a note of the IP address, and then add an A record to your DNS associating the name demo.timpsongray.com with that IP address. You may need to wait a little while for the DNS changes to propogate.\nCreate a Secret The application uses either environment variables or docker secrets to configure itself. From an internal perspective, this distinction is abstracted away with the use of the walmartlabs/dyn-edn Clojure library.\nHowever, GKE adds a feature to its use of secrets that is not available with docker swarm - it\u0026rsquo;s possible to have a secret\u0026rsquo;s value dynamically injected into a container\u0026rsquo;s environment as a standard environment variable.\nIn order to make use of this, of course, you must create a secret. This can be done as follows. (The value of \u0026lt;THESECRET\u0026gt; should be the password for the keystore used by the Jetty instance in your application).\n$ kubectl create secret generic \\\rallocations-app-v1-secrets \\\r--from-literal=ALLOC_KEYSTORE_PASSWORD='\u0026lt;PASSWORD\u0026gt;' \\\r--from-literal ALLOC_SESSION_STORE_KEY='\u0026lt;16 byte session key\u0026gt;'\r You can check that the secret was created successfully by issuing the following command and inspecting the results\n$ kubectl describe secrets/allocations-app-v1-secrets\r It\u0026rsquo;s important that the length of the ALLOC_SESSION_STORE_KEY value is precisely 16 bytes.\nDeploy your App to the Cluster Now deploy the Docker image containing the application to a container running in the cluster specifying the image recently pushed to the Google registry.\nDuring the deployment GKE will be requested to inject the values of the ALLOC_KEYSTORE_PASSWORD and ALLOC_SESSION_STORE_KEY from the allocations-app-v1-secrets resource as an environment variable (also called ALLOC_KEYSTORE_PASSWORD and ALLOC_SESSION_STORE_KEY respectively) into the container\u0026rsquo;s run-time environment. These variables are used by the Clojure application to gain access to Jetty\u0026rsquo;s keystore, which is required to allow Jetty to publish the application on an https endpoint; and the key used to encode session cookies.\n\rKubernetes secrets can also be made available within the container at a particular mount point (using tmpfs). This is similar to Docker swarm\u0026rsquo;s strategy. We could use it here, but the environment variable approach is simpler and the use of the dyn-edn library ensures that there\u0026rsquo;s very little transition to be done moving from a local development environment and the Kubernetes production environment.\r\r\rNow instruct GKE to deploy the application using\n$ kubectl apply -f deploy.yaml\r where the contents of the deploy.yaml file is as follows\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rapp: allocations-app-v1\rname: allocations-app-v1-web\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: allocations-app-v1\rtier: allocations-app-v1-web\rtemplate:\rmetadata:\rlabels:\rapp: allocations-app-v1\rtier: allocations-app-v1-web\rspec:\rcontainers:\r- image: gcr.io/allocations-accounting-v1/allocations-accounting:v1.0\rname: allocations-app-v1-app\rports:\r- containerPort: 8081\renv:\r- name: ALLOC_HOST_NAME\rvalue: demo.timpsongray.com\r- name: ALLOC_KEYSTORE_PASSWORD\rvalueFrom:\rsecretKeyRef:\rkey: ALLOC_KEYSTORE_PASSWORD\rname: allocations-app-v1-secrets\r- name: ALLOC_SESSION_STORE_KEY\rvalueFrom:\rsecretKeyRef:\rkey: ALLOC_SESSION_STORE_KEY\rname: allocations-app-v1-secrets\r Note that the env in the deployment yaml file also specifies a value for ALLOC_HOST_NAME. This is important as the application will make decisions about what ports to use for serving content and api endoints based on this value.\nIn the current codebase, if the host name ends with the string \u0026ldquo;timpsongray.com\u0026rdquo; then all communication is assumed to occur on port 80. This is probably what\u0026rsquo;s intended for a production system served using https. Obviously, your site name will be different and you should adjust the code.\nCreate a Back-End Service To access the deployed application GKE is requested to create a back-end service over the pods containing the deployment. The request will be for a NodePort service, which exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using :. Superset of ClusterIP.\nThis is done using\n$ kubectl apply -f service.yaml\r where the content of the service.yaml file is as follows\napiVersion: v1\rkind: Service\rmetadata:\rname: allocations-app-v1-svc\rannotations:\rcloud.google.com/app-protocols: '{\u0026quot;app-https-port\u0026quot;:\u0026quot;HTTPS\u0026quot;,\u0026quot;app-http-port\u0026quot;:\u0026quot;HTTP\u0026quot;}'\rlabels:\rapp: allocations-app-v1\rspec:\rtype: NodePort\rselector:\rapp: allocations-app-v1\rtier: allocations-app-v1-web\rports:\r- name: app-https-port\rport: 8081\rtargetPort: 8081\r- name: app-http-port\rport: 8080\rtargetPort: 8080\r \rWhy http? (and other notes on Health Checks) GKE will automatically create Health Checks to check the status of the backend services created, and which expose your deployment.\nBy default, for web services, GKE will probe the app at a particular path (/ or /healthz) using a particular protocol (http or https).\nNetwork load balancers require legacy health checks. These must be http, which means that the backend must support http probing by the health checking mechanisms. Don\u0026rsquo;t disable http on NodePort service (the backend service) or GKE will complain.\n Although Legacy health checks can be https, the Network Load Balancer only supports http.\n \r\rCheck the Service\u0026rsquo;s Status A convenient way to check if a web application is running correctly is to use port forwarding from your local machine to tunnel directly to the running pod. You can use the following command to open a tunnel to the node\u0026rsquo;s port 8081 (which is the home port for the containerized Clojure application) from port 8080 on localhost.\n$ gcloud container clusters get-credentials \\\rallocations-accounting-v1-cluster --zone us-east4-a --project allocations-accounting-v1 \\\r\u0026amp;\u0026amp; kubectl port-forward $(kubectl get pod \\\r--selector=\u0026quot;app=allocations-app-v1,tier=allocations-app-v1-web\u0026quot; \\\r--output jsonpath='{.items[0].metadata.name}') 8080:8081\r and then in your browser, navigate to https://localhost:8080.\nIf everything is operating correctly, you should see the home page of the Clojure application served by Jetty.\nIn your console window type Ctrl+C to stop port forwarding.\nSet up External Routing In the following section we will connect our chosen URL demo.timpsongray.com to the application.\nBut first we\u0026rsquo;ll need to perform a few checks and actions.\nCheck that the app\u0026rsquo;s DNS name is available From the command line run\n$ nslookup demo.timpsongray.com\r and ensure that the address returned is the static IP address that was created by Google earlier. This indicates that the DNS is responding correctly.\nCreate a Google Managed SSL Certificate We want to use https on our publicly accessible endpoint so we\u0026rsquo;ll need to install an SSL certificate. There are a few ways to do this, but the most convenient is to use GCP\u0026rsquo;s managed SSL certificates.\n\rDNSSEC\nIn order for the managed certicate creation to happen correctly, and for the external IP address you provisioned to be associated with it (when you create the Ingress service), DNSSEC must be enabled on your domain and the A record you created on the domain must point to the static IP address.\nIf either of these aren\u0026rsquo;t set correctly, you may see a Status: FailedNotVisible status when you issue the kubectl describe managedcertificate command below, and the Ingress creation will fail.\n\r\rWe can request a managed SSL certificate using\n$ kubectl apply -f cert.yaml\r where the content of the cert.yaml file is\napiVersion: networking.gke.io/v1beta1\rkind: ManagedCertificate\rmetadata:\rname: allocations-app-v1-cert\rspec:\rdomains:\r- demo.timpsongray.com\r Check the status of the SSL certificate We can check the status of the SSL provisioning process using\n$ gcloud compute ssl-certificates list --global\r which will show something like\nNAME TYPE CREATION_TIMESTAMP EXPIRE_TIME MANAGED_STATUS\rmcrt-5fc3491d-8eb3 MANAGED 2020-05-28T06:38:28.757-07:00 PROVISIONING\rdemo.timpsongray.com: PROVISIONING\r indicating that provisioning has started, and that the URL is as expected.\nCreate an Ingress Front-End Service In order to connect the outside world with the back-end service we will create load-balanced Ingress service using\n$ kubectl apply -f ingress.yaml\r where the content of the ingress.yaml file is\napiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rname: allocations-app-v1-web\rannotations:\rkubernetes.io/ingress.global-static-ip-name: \u0026quot;allocations-app-v1-addr\u0026quot;\rkubernetes.io/ingress.allow-http: \u0026quot;false\u0026quot;\rnetworking.gke.io/managed-certificates: \u0026quot;allocations-app-v1-cert\u0026quot;\rlabels:\rapp: allocations-app-v1\rspec:\rbackend:\rserviceName: allocations-app-v1-svc\rservicePort: 8081\r Although the kubectl command returns quickly it can take a few minutes for the ingress to be provisioned, deployed and stabilized. You can check its status using\n$ kubectl describe ingress allocations-app-v1\r When the provisioning is completed, you should be able to navigate to https://demo.timpsongray.com and view your application.\nViewing Logs Kubernetes allows you to inspect the logs of the Clojure application if you specify the pod in which it\u0026rsquo;s running. In order to discover the pod name you can issue the following command\n$ kubectl get pods\r which will return something like\nNAME READY STATUS RESTARTS AGE\rallocations-app-v1-web-5d966f5d8-v2wgn 1/1 Running 0 32m\r You can then issue the following command (substituting the correct pod name) to view the application logs\n$ kubectl logs allocations-app-v1-web-5d966f5d8-v2wgn\r Closure Deploying a web application to Kubernetes and exposing it on the web with a specific URL isn\u0026rsquo;t particularly difficult, but there are a number of places where things can go pear-shaped. Hopefully, this will help when you try to do the same thing.\n","date":1590659384,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593110503,"objectID":"16016e5b63860e4d20373718f4684f5a","permalink":"https://heykieran.github.io/post/deploy-to-kubernetes-gke/","publishdate":"2020-05-28T05:49:44-04:00","relpermalink":"/post/deploy-to-kubernetes-gke/","section":"post","summary":"I will show you how to deploy a packaged application comprising a Clojure Pedestal API server and a ClojureScript (reagent/reframe) front-end React application to a Kubernetes Cluster running on GKE (Google), and how to make it available externally at a specific URL.","tags":["clojure","deployment","kubernetes","GKE"],"title":"Deploy a Clojure Web Application to Kubernetes (GKE)","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction In this post I\u0026rsquo;ll show how to build and deploy to docker a fully-functioning application comprising a secure Pedestal API web-server and a React front-end application written in ClojureScript which accesses the server.\nFor this example I\u0026rsquo;ll be using the Pedestal/React application I previously discussed in this blog post (with its code here for tag v1.0), and the code discussed in this post is available here.\nOutline of the Steps I\u0026rsquo;ll set up a working directory for the build, clone the target application into a sub-directory, compile the target, package it and its dependencies to java byte code, assemble them into a jar, create a docker image for the application, and deploy it as a docker service.\nBackground, Challenges \u0026amp; Tools During this exercise, I\u0026rsquo;ll try to keep separate the application I\u0026rsquo;m building from the application doing the building. This isn\u0026rsquo;t strictly necessary but it will help illustrate a procedure generally applicable to any Clojure application.\nOne of the challenges of this approach is that I\u0026rsquo;ll need to deal with two deps.edn files - one for the build environment and one for the application being built. Each deps file contains relative paths (:paths and :extra-paths) relative to the root of the project directory to which it belongs.\nIf I am to avoid changing in any way the deps file for the project being built and yet still ensure that the built artifacts end up in the correct location within the build project\u0026rsquo;s directory structure I will need a way to inform the compiler about which paths to use, but relative to the build project\u0026rsquo;s directory and not as specified in the target\u0026rsquo;s deps.edn file.\nAs an example, let\u0026rsquo;s suppose that the build project is at ./clj-deploy-docker and the project being built will be cloned into ./clj-deploy-docker/target-app.\nThe deps.edn file in ./clj-deploy-docker/target-app will contain an entry for the alias :main as below\n:aliases\r{:main\r{:paths [\u0026quot;src\u0026quot;]\r:extra-deps {ch.qos.logback/logback-classic {:mvn/version \u0026quot;1.2.3\u0026quot;}\rorg.clojure/tools.logging {:mvn/version \u0026quot;0.4.1\u0026quot;}\rring/ring-core {:mvn/version \u0026quot;1.8.0\u0026quot;}\rring/ring-jetty-adapter {:mvn/version \u0026quot;1.8.0\u0026quot;}\rring/ring-devel {:mvn/version \u0026quot;1.8.0\u0026quot;}\rio.pedestal/pedestal.service {:mvn/version \u0026quot;0.5.7\u0026quot;}\rio.pedestal/pedestal.route {:mvn/version \u0026quot;0.5.7\u0026quot;}\rio.pedestal/pedestal.jetty {:mvn/version \u0026quot;0.5.7\u0026quot;}\rbuddy {:mvn/version \u0026quot;2.0.0\u0026quot;}\rhiccup {:mvn/version \u0026quot;1.0.5\u0026quot;}\rorg.conscrypt/conscrypt-openjdk-uber {:mvn/version \u0026quot;2.2.1\u0026quot;}\rorg.eclipse.jetty/jetty-alpn-conscrypt-server {:mvn/version \u0026quot;9.4.24.v20191120\u0026quot;}\rcom.google.api-client/google-api-client {:mvn/version \u0026quot;1.30.6\u0026quot;}\rcom.walmartlabs/dyn-edn {:git/url \u0026quot;https://github.com/walmartlabs/dyn-edn.git\u0026quot; :sha \u0026quot;855a775959cf1bec531a303a323e6f05f7b260fb\u0026quot;}}\r:extra-paths [\u0026quot;resources\u0026quot; \u0026quot;common-src\u0026quot; ]}\r In order to access and use this alias correctly from the build project\u0026rsquo;s directory (clj-deploy-docker) I will need to adjust (in some way) the paths so that the compiler is operating with the correct class path i.e. the class path of the target rather than the class path of the build. Therefore, I\u0026rsquo;ll need to let the compiler know (in some way) that the :paths and :extra-paths vectors should read\n:paths [\u0026quot;target-app/src\u0026quot;]\r and\n:extra-paths [\u0026quot;target-app/resources\u0026quot; \u0026quot;target-app/common-src\u0026quot; ]\r respectively.\n On the other hand, the maven coordinates in the target\u0026rsquo;s deps.edn file are correct, so we can leave the :deps and :extra-deps values as they are found.\n As for the \u0026ldquo;in some way\u0026rdquo;, I will be using the Badigeon library to achieve this. Many of Badigeon\u0026rsquo;s API\u0026rsquo;s take a :deps-map as input. This is an in-memory map whose structure is the same as a deps.edn file. This will allow me to read the deps file, make in-memory adjustments and feed it to to API to do the bundling and compiling with a classpath relative to any directory I choose (i.e. relative to ./clj-deploy-docker).\nCreate a Working Directory for the Project Create a working directory for the project, and cd into it\n$ mkdir clj-deploy-docker\r$ cd clj-deploy-docker\r Setting up the Application to be built Now, I\u0026rsquo;ll clone the repository of the application I want to build into a directory target-app under my working directory.\n$ git clone https://github.com/heykieran/clj-pedestal-google.git target-app\r As mentioned above, for this exercise I will be using a library called Badigeon to organize and compile the sources. It leverages many of the tools \u0026amp; libraries already available in clojure.core and tools.deps; it\u0026rsquo;s very flexible and I find the API intuitive.\nCreating the build runner In my project\u0026rsquo;s working directory I create a deps.edn file.\n$ touch deps.edn\r and add the Badigeon dependency to the deps.edn file.\n{:deps {}\r:aliases\r{:build\r{:extra-paths [\u0026quot;build\u0026quot;]\r:extra-deps\r{badigeon/badigeon\r{:git/url \u0026quot;https://github.com/EwenG/badigeon.git\u0026quot;\r:sha \u0026quot;1edf7ae465db870ec0066f28226edb9b04873b70\u0026quot;\r:tag \u0026quot;0.0.11\u0026quot;}}}}}\r Apart from the Clojure system and user dependencies this is the only dependency I\u0026rsquo;ll need in that file.\nAlso, for later use, I create a directory called build to contain the Clojure files to run the bundling, compilation and assembling processes.\n$ mkdir build\r Building the Front-End JS File As outlined in my previous blog post, the following command will build the front-end production application\u0026rsquo;s js file from the ClojureScript sources for the application being dockerized.\n$ cd target-app\r$ clj -A:prod\r$ cd ..\r This will build the application\u0026rsquo;s front-end only and place the production js file (prod-main.js) in the target-app/target/public/cljs-out directory. This is the only change that will be made to the directories and files under target-app.\nAt a later stage I will move this file to its correct location under my project directory (./clj-deploy-docker) so that it can be included in the docker image.\nBuilding the Back-End (JVM) Class Files I\u0026rsquo;ll now cd into the build directory I created previously and create a package.clj file. This file will contain the -main method that ultimately performs the bundling, compilation and consolidation of the back-end Clojure files i.e. the JVM class files.\nSome Background on Bundling, Compilation and Consolidation (Jar\u0026lsquo;ing) There are three distinct phases to assembling the JVM artifacts to include in the docker image and I will be using the Badigeon API to perform all three phases.\n  Bundling The bundling step creates a \u0026ldquo;bundle\u0026rdquo; at a specified file-system location of all the target project\u0026rsquo;s resources and dependencies, including any jar files that are needed. Note that because the Badigeon bundler does not merge in the system and user deps preferences, it will not automatically copy sources that are in src directory of your project, unless that directory is explicitly specified in the :paths or :extra-paths entries in the deps.edn file. During the bundling phase all the jar files required by your application, and all other resources on the classpath such as static html file, css files, user authored js files etc. will be copied to the specified target directory.\n  Compilation During the compilation step the compiled versions of your Clojure source files (as .class files) are generated and copied to a specified target directory.\n  Consolidation The final phase involves creating a jar file containing all the .class files needed by the application with an appropriate manifest file (META-INF/MANIFEST.MF) which has an entry indicating the application\u0026rsquo;s entry-point (a Main-Class entry), and an entry specifying the libraries to be used by the jar file (a Class-Path entry).\n Dependencies (found by Badigeon using the :deps and :extra-deps coordinates) will not be incorporated into this jar file. They will however be added to a ./lib directory as individual jar files and referenced by the Class-Path entry in the jar\u0026rsquo;s manifest file.\n   Once these three phases are complete, and the js file containing the front-end application is placed in its correct location, the application can be run using the java command line tool.\nI\u0026rsquo;ll come to that presently, but first I\u0026rsquo;d like to take a slightly deeper look at the bundling, compilation and consolidation phases. The full details are available in the package.clj file from which the following code snippets have been extracted.\nNotes on the code performing the three steps First, I bundle\n(bundle\rout-path\r{:deps-map translated-deps-map\r:aliases aliases\r:libs-path \u0026quot;lib\u0026quot;})\r Given a deps-map and a vector of aliases ([:main]) this function will copy the projects\u0026rsquo;s resources needed to out-path, and also copy the jar files required to out-path/lib. Because the code I want to bundle is in the target-app directory, I\u0026rsquo;ll read the deps.edn file from its location under target-app and update the :path and :extra-paths entries so that they are now relative to the current working directory rather than target-app (see above).\nNow the compilation phase runs:\n(compile/compile\r'main.core\r{:compile-path\rclasses-path\r:classpath\r(translate-path-to-absolute\rtarget-dir\rdeps-map\raliases)})\r This compiles the main.core namespace, putting the .class files into the directory specified by the classes-path directory, using a classpath specified by the value of the :classpath entry. In my case, this is generated by reading the target-app/deps.edn file into deps-map and converting the relative components of :paths and :extra-paths vectors to absolute file-system locations.\nFinally, the consolidation phase runs:\n(spit manifest-path\r(jar/make-manifest\r'main.core\r{:Class-Path\r(str\r\u0026quot;. \u0026quot;\r(str/join\r\u0026quot; \u0026quot;\r(mapv\r#(str \u0026quot;lib/\u0026quot; (.getName %))\r(.listFiles (io/file \u0026quot;target/app/lib\u0026quot;)))))}))\r(zip/zip\rclasses-path\r(str (make-path out-path \u0026quot;app-runner\u0026quot;) \u0026quot;.jar\u0026quot;))\r This achieves two things:\n  It creates a manifest file in target/app/classes/META-INF, setting main.core as the entry point, and adds entries for all the jar files in the target/lib directory (which was created and populated during bundling) into the manifest file\u0026rsquo;s Class-Path header field. In order for the application to run there is an assumption that the final jar file and the lib directory will exist at the same level in the file system i.e. in the same directory.\n  It creates a app-runner.jar file from the contents of the target/app/classes directory. This jar file is the main application and will contain the manifest file just created with its Class-Path entry pointing to the non-application jar files it needs to run - i.e. those found in the lib directory.\n  In order to run all three steps, I can \u0026ldquo;execute\u0026rdquo; the package namespace passing the target-app directory name as an argument.\n$ clj -A:build -m package \u0026quot;target-app\u0026quot;\r This completes the Bundling, Compilation and Consolidation steps, and when it finishes I will have a directory structure, which with the addition of the front-end js file (which I compiled above) will constitute the complete application.\nThe result is a target folder containing an app-runner.jar file and any other supporting files needed to run the app. Many are extraneous; for instance all the classes files, now included in the jar file are also under this directory, as are the source code of the Clojure files.\nI can copy the js file to its correct location using\n$ mkdir -p target/app/public/cljs-out \u0026amp;\u0026amp; \\ cp target-app/target/public/cljs-out/prod-main.js \u0026quot;$_\u0026quot;\r Now, everything I need (and some I don\u0026rsquo;t) is available in the ./target/app/ directory.\nRunning the Application Before running the compiled application I need to ensure that certain environment variables are defined and set correctly.\nAs discussed in my previous post, the application requires a number of environment variables to be set in order to configure itself correctly.\nThese are\n# the https port number used by the Pedestal API server\rALLOC_SSL_PORT=8081 # the password of Jetty's keystore ALLOC_KEYSTORE_PASSWORD=\u0026lt;password\u0026gt; # the http port number used by the Pedestal API server\rALLOC_PORT=8080 # the file system location of the Jetty's keystore (as an absolute file path)\rALLOC_KEYSTORE_LOCATION=\u0026lt;location\u0026gt;  I can cd into the built artifact\u0026rsquo;s directory (./target/app) and run the backend application directly from the jar file\n$ cd target/app\r$ java -jar app-runner.jar\r or, because the classes still exist in a classes directory under the app directory\n$ cd target/app\r$ java -cp .:classes:lib/* main.core\r The app will start, and when it\u0026rsquo;s fully initialized, I can navigate to https://localhost:8081/r/home to see it in action.\n There is also a lot of unnecessary \u0026ldquo;residue\u0026rdquo; in the ./target/app directory, created during bundling, including directories containing clj and cljc files that are not actually needed to run the application (they will already have been compiled into the classes directory).\n When I process the files for deployment to docker, these will be removed.\nThere remains then only the task of creating the docker image itself, which is outlined below.\nQuick Review Currently, we have in the target/app all the artifacts (with some extras) to run the application. Now we will rationalize those artifacts, removing all the unnecessary ones, leaving only those that are necessary for running our application and package what remains into a docker image, which we\u0026rsquo;ll place in the folder docker/deploy.\nCreate the Docker Image The docker image I will use is very simple - a basic Debian stretch image with a Java8 SDK.\nIn my project directory I create a directory called docker and cd into it.\n$ mkdir docker\r$ cd docker  and create a Dockerfile containing\nFROM openjdk:8-stretch\rCOPY entrypoint.sh /sbin/entrypoint.sh\rRUN chmod 755 /sbin/entrypoint.sh\rEXPOSE 8081/tcp\rCOPY deploy /image\rWORKDIR /image/app\rENV ALLOC_KEYSTORE_LOCATION=/image/local/jetty-keystore \\\rALLOC_KEYSTORE_PASSWORD=password \\\rALLOC_PORT=8080 \\\rALLOC_SSL_PORT=8081 ENTRYPOINT [\u0026quot;/sbin/entrypoint.sh\u0026quot;]\r The Dockerfile as defined will\n create an image from a base openjdk:8-stretch image, copy the file entrypoint.sh (which I haven\u0026rsquo;t created yet) from the docker/deploy folder to the image\u0026rsquo;s /sbin directory and set its mode to executable, enable network connectivity to port 8081 only (there will be no access to the unprotected http port 8080), copy the entire contents of the docker/deploy directory to the image\u0026rsquo;s /image directory, set the image\u0026rsquo;s working directory to /image/app, set the needed environment variables for the new image, and specify that the /sbin/entrypoint.sh script should be run when the container starts.  The deploy directory under the docker directory is the location where the application\u0026rsquo;s artifact will be assembled before their inclusion in the image when the COPY deploy /image command is run.\nFrom my project\u0026rsquo;s folder (clj-deploy-docker) I run the following command to copy the entire app (including residue) to the docker/deploy folder (creating it if it doesn\u0026rsquo;t exist).\n$ mkdir -p docker/deploy/app \u0026amp;\u0026amp; cp -r target/app/* \u0026quot;$_\u0026quot;\r Now, in order to remove the extraneous files I can run\n$ find docker/deploy/app -maxdepth 1 -mindepth 1 -type d \\( ! \\( -name 'lib' -o -name 'public' \\) \\) -exec rm -rf {} \\;\r This deletes any sub-directory in the docker/deploy folder not named lib (which contain the jar files the application needs) or public (which contains all the non-JVM resources the application needs).\nI now add the entrypoint.sh file to the docker folder. This script, which is run when the image is started, simply calls the application\u0026rsquo;s entry-point.\n#!/bin/bash\r# exit immediately if error\rset -e\rjava -jar app-runner.jar\r Finally, I need to ensure that the keystore used to encrypt the application\u0026rsquo;s https communication is available for the image build process, so I copy it from my local file system\u0026rsquo;s location to the /docker/deploy/local directory, from whence, during the image building process, it will be copied to the image\u0026rsquo;s /image/local folder.\n# mkdir -p docker/deploy/local \u0026amp;\u0026amp; cp \u0026lt;location-of-keystore\u0026gt; \u0026quot;$_\u0026quot;\r Now, with everything cleaned-up and with the script and the keystore in place, I can create the application\u0026rsquo;s docker image, tagging it with the label testapp:dev.\n$ docker build -t testapp:dev docker\r and then run a container based on that image using\n$ docker run --rm --name test --env ALLOC_KEYSTORE_PASSWORD=\u0026lt;the-real-keystore-password\u0026gt; -p:8081:8081 -it testapp:dev\r I can then open my browser to https://localhost:8081/r/home in order to confirm it\u0026rsquo;s running correctly.\n Note When I created the image I did not include the correct password for the keystore in the Dockerfile. Therefore, in order for the application to work correctly I\u0026rsquo;m required to pass the correct value by setting the env variable ALLOC_KEYSTORE_PASSWORD when I start the container. It will be used in lieu of the value embedded in the image.\n Docker Secrets Passing sensitive information using environment variables is satisfactory in many situations, but there is available a better approach: docker secrets.\n Note Docker secrets are not available in stand-alone mode, the feature is only available in swarm mode.\n Passing Configuration Values For further details on the subject of passing configurations to a Clojure application you can refer to my blog post on the subject. The post also discusses more fully the mechanics of how the configuration is used by the application\u0026rsquo;s code.\nCreate a swarm To create a local swarm for testing I can issue the following command\n$ docker swarm init\r Once the swarm has been initialized I can add a secret to the registry. Let\u0026rsquo;s suppose I want to protect the ALLOC_KEYSTORE_PASSWORD and avoid having to pass it to the image as an environment variable. I can simply create a docker secret to hold the value, protecting it from being stolen too easily. The following command will create a secret called ALLOC_KEYSTORE_PASSWORD, set its value to MYKEYSTOREPASSWORD and store it in the swarm\u0026rsquo;s registry.\n$ printf \u0026quot;MYKEYSTOREPASSWORD\u0026quot; | docker secret create ALLOC_KEYSTORE_PASSWORD -\r You can test that the secret was successfully created by issuing\n$ docker secret ls\r In order to use the secret, the container has to be started as a service within the swarm, and on the command line must be specified to what secrets the service has access. In order to start the container with access to the ALLOC_KEYSTORE_PASSWORD and linking swarm\u0026rsquo;s network to the host\u0026rsquo;s network I can issue the following command\n$ docker service create --replicas 1 \\\r--secret ALLOC_KEYSTORE_PASSWORD \\ --name testapp \\\r--publish mode=host,published=8081,target=8081 \\\rtestapp:dev\r This will start the service (named testapp) within the swarm, and the service will start serving the application similarly to when I used the docker run command above.\nOnce started the following command will return basic information about the service\n$ docker service ls\r and this information should look something like the following\nID NAME MODE REPLICAS IMAGE PORTS\rkjzh1uc2ttng testapp replicated 1/1 testapp:dev  If I want to monitor the activity of the service I should monitor the logs of its associated container and, in order to do this I need to know the container\u0026rsquo;s ID.\nI can issue the following command and note the value in the CONTAINER_ID column for the image testapp:dev and use it to interrogate the logs.\n$ docker container ls\r This will return something like the following:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\rd5b269d508b4 testapp:dev \u0026quot;/sbin/entrypoint.sh\u0026quot; 15 seconds ago Up 14 seconds 0.0.0.0:8081-\u0026gt;8081/tcp testapp.1.tb4n70oe1t8tz3qdzo2aawmek\r And I can view the logs of the running container using as much of the container\u0026rsquo;s ID as necessary to make it unique\n$ docker logs d5b\r This allows me to confirm that the application started correctly and is responding to requests.\nAs before, I can point my browser at https://localhost:8081/r/home and exercise the packaged application running as a docker service.\nAfter some activity I can review the history of my interactions (and the server\u0026rsquo;s responses) by once again reviewing the logs:\n$ docker logs d5\r To shutdown the service, I run\n$ docker service rm testapp\r Review There were quite a number of steps but I hope the detail was illuminative.\nA later post will illustrate how to integrate the build process within the Clojure application directory structure rather than requiring that it be cloned into a separate working directory.\nThat post will also show how the build and deployment steps can be automated using a simple Makefile.\n","date":1588867826,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592760342,"objectID":"a45d8c4bce10c3363965d7dbe099dde0","permalink":"https://heykieran.github.io/post/deploy-pedestal-react-to-docker/","publishdate":"2020-05-07T12:10:26-04:00","relpermalink":"/post/deploy-pedestal-react-to-docker/","section":"post","summary":"I will demonstrate how to compile and package a completely operational, but minimal, application comprising a secure Clojure Pedestal API server and a ClojureScript (reagent/reframe) front-end React application; and finally deploy that application to a docker container running as a Docker swarm service with its configuration provided by Docker's secrets functionality.","tags":["clojure","clojurescript","pedestal","react","docker"],"title":"Deploy a Clojure Pedestal API Server \u0026 React/ClojureScript Web Application to Docker","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Passing a \u0026ldquo;Configuration\u0026rdquo; to a Clojure Application Introduction There are many ways to pass configuration values to a Clojure application. This piece will cover four of them:\n \rCommand Line Parameters \rEnvironment Variables \rDynamic Environment Variables, and \rDocker Secrets  The first two are briefly discussed, while greater time is spent on the final two. Of the four, the last is particularly useful to keep secure configuration values that ought to be kept so - passwords, private keys etc.\nUsing Command Line Parameters If one starts Clojure from the command line using the -m option specifying a namespace, Clojure will execute the -main function from that namespace, passing any further arguments on the command line as parameters to -main.\nFor example, the following Clojure code\n(ns main.core)\r(defn -main [args]\r(println args))\r which can be executed from the command line using\n$ clj -m main.core \u0026quot;Hello World!\u0026quot;\r will result in the string Hello World! being printed to the console.\nUsing Environment Variables As an alternative to command line parameters, it\u0026rsquo;s often convenient to have your Clojure application read its parameters from the application\u0026rsquo;s execution environment i.e. environment variables or JVM system properties.\nSo running\n$ export MYARGS=\u0026quot;Hello World!\u0026quot;\r at the command line, and changing the -main function to\n(defn -main [\u0026amp; args]\r(println (System/getenv \u0026quot;MYARGS\u0026quot;)))\r you can now run the application using\n$ clj -m main.core\r and see the same result.\nThe value of the MYARGS environment variable is read from the environment and then printed to the console.\nUnfortunately, as convenient as this is when executing the code, it can be a little inconvenient during development. If this is the only place you use the variable there\u0026rsquo;s little lost, but if the value is used in other areas of your application e.g. in other namespaces, any changes to its name or expected type will lead to an amount of error-prone \u0026ldquo;code surgery\u0026rdquo;.\nAlso, env variables are, by their nature, strings; so if you need the value as, for instance, an int you\u0026rsquo;ll need to perform the casting and error-checking at the time of initialization.\nUsing \u0026ldquo;Dynamic\u0026rdquo; Environment Variables WalmartLabs have published a Clojure library on GitHub to address many of these issues. The library centralizes the reading of env variables, and also allows for existence-checking, the setting of default values, merging with JVM system properties, casting, type-checking, and composition.\nThe library makes it possible to define in a simple edn file the shape of your configuration data and have it parsed correctly from the environment (and other locations) into the structure you want.\nAs an example, if you have a file called config.edn somewhere on your classpath with\n{:app-configuration\r{:myargs #dyn/prop MYARGS}}\r and change the main/core.clj file to\n(ns main.core\r(:require\r[clojure.edn :as edn]\r[clojure.java.io :as io]\r[com.walmartlabs.dyn-edn :refer [env-readers]]))\r(def app-config\r(-\u0026gt;\u0026gt; \u0026quot;config.edn\u0026quot;\rio/resource\rslurp\r(edn/read-string {:readers (env-readers)})))\r(defn -main [\u0026amp; args]\r(println (get-in app-config [:app-configuration :myargs])))\r and then run the application using\n$ clj -m main.core\r You\u0026rsquo;ll see the same result - but, the application\u0026rsquo;s configuration has been correctly (and automatically) parsed into a configuration structure and is available as a map named main.core/app-config that can be used throughout your application.\nThe use of the config.edn file also allows you to view the expected configuration parameters, or add to them, or change their default values in one central location - very convenient.\nUsing Docker Secrets - with Dynamic Environment Variables An area where env variables are extensively used as configuration parameters is when an application is being run inside a docker container. By providing one or more -e options to the docker run command, it\u0026rsquo;s possible to establish the configuration environment for the application (if that\u0026rsquo;s where the application expects to find it).\nUnfortunately, certain configuration parameters contain sensitive information, such as passwords or private keys and one can\u0026rsquo;t realistically embed those values in the application\u0026rsquo;s code. They may change frequently; they may need to differ from one container to another; and their very existence in the code represents a risk that they\u0026rsquo;ll \u0026ldquo;leak\u0026rdquo; into an SCM.\nOf course, the use of environment variables is a good alternative to embedded code values, but represents a different, albeit smaller, set of risks. Anyone with access to the docker instance could recover the environment variables passed to a container during initialization.\nIn order to address this, Docker introduced the concept of secrets with docker swarm. Secrets allow sensitive information to be defined securely, and then selectively made available to containers which are running as docker services. It is only within the running container that the secret\u0026rsquo;s value is available as contents of files mounted from an in-memory filesystem, by default at /run/secrets/\u0026lt;secret_name\u0026gt;, where they can be accessed by the application.\nIn order to tie together environment variables with secrets, I\u0026rsquo;ve submitted a PR to the maintainer of the walmart-labs/dyn-edn library which, in addition to env variables and system properties, merges in docker secrets to the set of variable available to the library\u0026rsquo;s readers: #dyn/prop, #dyn/join, #dyn/long, #dyn/boolean, and #dyn/keyword.\n Note The PR was accepted by the maintainer, but the library hasn\u0026rsquo;t yet made it to clojars. In order to use the secrets functionality you\u0026rsquo;ll need to add the following to your :deps entry in deps.edn. This will pull the appropriate version of the code.\n com.walmartlabs/dyn-edn {:git/url \u0026quot;https://github.com/walmartlabs/dyn-edn.git\u0026quot;\r:sha \u0026quot;855a775959cf1bec531a303a323e6f05f7b260fb\u0026quot;}\r Our Example with Secrets To use a docker secret in lieu of the MYARGS env variable used in previous examples all one needs to do is create a secret called MYARGS with the appropriate value\n$ printf \u0026quot;Hello World!\u0026quot; | docker secret create MYARGS -\r and, when starting the container as a docker service, authorize the service to use that secret\n$ docker service create --replicas 1 --secret MYARGS --name \u0026lt;svcname\u0026gt; \u0026lt;image containing the app\u0026gt;\r No change needs to be made to the config.edn file, or to the source code.\n","date":1588865175,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590701972,"objectID":"3c8487f2b01a3ae253af0157c7f25bdc","permalink":"https://heykieran.github.io/post/clojure-configuration/","publishdate":"2020-05-07T11:26:15-04:00","relpermalink":"/post/clojure-configuration/","section":"post","summary":"A piece that discusses options for passing configuration information to Clojure applications. It covers alternatives ranging from simple command line parameters to the use of Docker secrets.","tags":["clojure","docker"],"title":"Clojure Configurations with Docker Secrets","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction The following are some notes about a repository containing working code (extracted from a larger project) demonstrating a combination of a secured Pedestal website (and associated API services), and a React-ive ClojureScript front-end application that utilizes either Google or bespoke login logic to identify and validate the user\u0026rsquo;s credentials, and sets his/her authorization levels.\nI hope that it may be helpful to anyone else who may know how each of the the individual pieces work, but is wondering how to put it all together.\nI owe a debt of gratitude to Tristan Straub, as much of the front-end logic (and code) to utilize Google\u0026rsquo;s login is based on some code he posted on Github. I\u0026rsquo;ve changed the code in many ways, so any errors are not his but mine.\nThe front-end application, which is intentionally simple, allows a user to login, and according to his/her permissions will allow access to various resources. The application is written using React/ReFrame, Semantic UI React and ClojureScript. The application uses Figwheel-main to compile and, in development mode run the front-end; but switching to a different tool-chain (e.g. shadow-cljs) should be relatively easy.\nFeatures Google Login Integration (and mapping to application id) The application demonstrates how to integrate Google\u0026rsquo;s login functionality with a ClojureScript application. After successfully authenticating with Google, the user\u0026rsquo;s Google email address is associated with one (and only one) internal application user ID. The internal user ID is associated internally with one or more application defined roles, which are defined in the code.\nConceivably, this mapping of external ID to internal ID could allow multiple external authentication services to be used to map multiple externally asserted identities to a single internal user ID. For example, by extending the application to use Facebook\u0026rsquo;s authentication service, it would be possible to have both user@gmail.com and user@facebook.com to be mapped to the same internal user ID, e.g. :user.\nFundamentally, authenticating and logging in merely associates a user with a web session. The session is the operative object and identities are not, and cannot be shared between sessions. A user may have multiple sessions open, but they don\u0026rsquo;t \u0026ldquo;mingle\u0026rdquo;.\nAd-hoc affirmative login method The application as presented allows a user to simply assert that they are a known user. The only reason this feature is included is to simplify debugging. In a production application these assertions would typically be replaced with an application specific logon process.\nIsolation of sensitive information from codebase In order to run Pedestal/Jetty (for production) or Figwheel/Jetty (for development) with https (required to use Google login) the location of a keystore and its password must be supplied.\nThis is an obvious security concern - including any sensitive information in either the source-code, or the application\u0026rsquo;s generated js code is poor security hygiene. The application avoids this by using environment variable (assumed to be available) to store this information which is read-only at runtime.\nSecured API end-points by role membership The application uses role-based security, where access to resources (URI\u0026rsquo;s) is permitted or prohibited according to whether a user has membership within a particular role. A user ID can be associated with one or more roles. Roles are independent of one another. There is no concept of hierarchy or inheritance beyond how the code chooses to handle these concepts.\nThe application, for our purposes, defines three roles: :admin, :user and :public. An unauthenticated user is associated with the role :public. Note that there is nothing privileged about these roles, or their names. They are completely application defined.\nIn the code for the application\u0026rsquo;s configuration file (common-src/config/config.cljc) you can see how these have been defined:\n(def roles-and-users\r{:admin {:roles #{:admin} :users #{\u0026quot;admin@timpsongray.com\u0026quot; \u0026quot;heykieran@gmail.com\u0026quot;}}\r:user {:roles #{:user} :users #{\u0026quot;user@timpsongray.com\u0026quot;}}})\r Here, we\u0026rsquo;ve defined two users :user and :admin, along with two roles, also called :user and :admin. Users who authenticated with the email addresses admin@timpsongray.com and heykieran@gmail.com will be associated with the user ID :admin, and the user with the email address user@timpsongray.com will be associated with the user ID :user.\nIf we examine how routes are defined in the file server/be_handler_pdstl.clj we can see how security is applied to URL\u0026rsquo;s.\n(def routes\r(route/expand-routes\r#{[\u0026quot;/echo\u0026quot; :get echo]\r[\u0026quot;/auth/isauthenticated\u0026quot; :post (build-secured-route-vec-to app-auth/get-current-logged-in-user) :route-name :alloc-public/is-authenticated]\r[\u0026quot;/auth/setid\u0026quot; :post (build-secured-route-vec-to app-auth/alloc-auth-explicitly-set-identity-of-user-post) :route-name :alloc-public/auth-set-id-post]\r[\u0026quot;/auth/google\u0026quot; :post (build-secured-route-vec-to app-auth/alloc-auth-google-login) :route-name :alloc-public/google-login-post]\r[\u0026quot;/auth/logout\u0026quot; :post (build-secured-route-vec-to disconnect-session) :route-name :alloc-user/auth-logout-post]\r[\u0026quot;/api/getsecresource/p\u0026quot; :post (build-secured-route-vec-to get-secured-resource) :route-name :alloc-public/test-res]\r[\u0026quot;/api/getsecresource/u\u0026quot; :post (build-secured-route-vec-to get-secured-resource) :route-name :alloc-user/test-res]\r[\u0026quot;/api/getsecresource/a\u0026quot; :post (build-secured-route-vec-to get-secured-resource) :route-name :test-res]\r[\u0026quot;/r/home\u0026quot; :get [content-neg-intc respond-with-app-page] :route-name :app-main-page]}))\r The current implementation uses the namespace of values of each route\u0026rsquo;s :route-name key to assign security.\nAny protected URL whose :route-name namespace is :alloc-public is available to any user, authenticated or not.\nAny protected URL whose :route-name namespace is :alloc-user is available to any user associated with the :user role.\nAny protected URL whose :route-name namespace is either :alloc-admin or the default namespace is available to only users associated with the :admin role.\n URL\u0026rsquo;s are only protected if they use the auth interceptors. These interceptors are included when the function build-secured-route-vec-to is used to wrap the content handler function.\n Another item to note is the three test URL\u0026rsquo;s /api/getsecresource/p (available to all users), /api/getsecresource/u (available to users in the :user role) and /api/getsecresource/a (available only to users in the :admin role) use the same handler get-secured-resource.\nDevelopment Server \u0026amp; Production Server The application has both a development mode and a production mode. Both modes use Pedestal as the API server, responding to requests as defined in the routes parameter used to start the server. The difference between the two modes is in how the js files are served, and in how front-end development proceeds, or not.\nIn development mode the application\u0026rsquo;s js files are served from a handler (fe-src/server/fe-handler) sitting behind figwheel/Jetty, and started using the script provided in scripts/server.clj. This facilitates the standard figwheel development process of having figwheel monitor a set of source directories and regenerate and reload any changed files as necessary. This should be familiar to anyone who\u0026rsquo;s used figwheel-main for ClojureScript development. An alias has been defined in the deps.edn file to start all the various servers and to start figwheel.\nIn production mode, the js files are served from the Pedestal/Jetty server itself. Of course, in order to do this the js files must have been previously compiled by figwheel. An alias has been defined in the deps.edn file for this purpose.\nLog-out Functionality The application allows the user to disassociate their session from their identity. Because the session is the operative object, this is essentially logging out.\nSession Expiration When credentials are issued for an authenticated user and associated with a session, the information will also contain an expiration date. If a user attempts to access a protected resource and the credentials are found to have expired, access is denied and the user\u0026rsquo;s credentials are disassociated from the session. This essentially logs that user out and he/she will need to reassociate their credentials with the session.\nReact/Reagent/Reframe/kee-frame Application The test application is a reactive application written in ClojureScript using reagent, reframe and kee-frame. It illustrates some of the principles required for a simple application of this type.\nSemantic UI Integration The toolkit used for widgets and styling is SemanticUI-react, and the application illustrates how the library can be used.\nRunning the Servers \u0026amp; Applications Setting up a keystore for HTTPS In order to run the web servers in secured mode you\u0026rsquo;ll need to create a keystore for the certificates used by the servers and make it available to Jetty. Instructions on how to do this can be found here.\nSetting up Google Login In order to use the application for yourself you will need to get your own Google Client ID. Instructions on how to do this can be found here.\nYou will also need to use the Google Console to inform Google of the Authorized Javascript Origins associated with your Client ID. These should be the names and ports of your https endpoints. For testing, these will typically be the server name and port of your Pedestal/Jetty and your figwheel/Jetty (for development mode only) servers.\nThe values should also be set in the following places\nIn your environment the following variables should be set\n   Environment Variable Value     ALLOC_KEYSTORE_PASSWORD The keystore\u0026rsquo;s password   ALLOC_KEYSTORE_LOCATION The keystore\u0026rsquo;s filesystem location   ALLOC_SSL_PORT The ssl port number used for Pedestal   ALLOC_PORT The port number used for Pedestal    In the common-src/config/config.cljc you will need to set the following variables\n   Configuration Variable Value     google-client-id your Google Client ID   my-hostname your server\u0026rsquo;s name   figwheel-ssl-port the port used by figwheel\u0026rsquo;s https server and serving the application\u0026rsquo;s js files   pedestal-port Pedestal\u0026rsquo;s HTTP port number (should match ALLOC_PORT).   pedestal-ssl-port Pedestal\u0026rsquo;s HTTPS port number (should match ALLOC_SSL_PORT)   google-callback-url change the server name in this variable to match your server\u0026rsquo;s name.    In the file dev.cljs.edn change the :open-url parameter to match your server\u0026rsquo;s name, and the ssl port used by figwheel. This should match https://\u0026lt;my-hostname\u0026gt;:\u0026lt;pedestal-ssl-port\u0026gt;/r/home.\nStarting the Server(s) Development Mode   In your IDE of choice, start a REPL (with the alias :main)\n  Load and execute the control namespace\n  Execute the function (start-dev) (It\u0026rsquo;s within a comment expression).\n  Log messages are sent to the REPL output stream, so you can monitor progress and activity.\n  When the Pedestal server has started, run the following from a command line\nclj -A:dev\r This will start the front-end server used by figwheel on ports 9500 and figwheel-ssl-port and will start the figwheel watch process.\n  Your browser should automatically open to https://\u0026lt;my-hostname\u0026gt;:\u0026lt;figwheel-ssl-port\u0026gt;/r/home where the application will be loaded. (You should have set this in dev.cljs.edn as above).\n  \rWhen you\u0026rsquo;re finished and wish to stop the front-end server: from the Figwheel console you issue the :cljs/quit command to stop the Figwheel build process followed by Ctrl+C to stop the front-end server itself.\r\r\rProduction Mode Build the production application by running\nclj -A:prod\r from the command line. This will generate the production js files from your ClojureScript sources. Then, from the command line run\nclj -A:main:main-output -m control\r This will start the Pedestal server which in addition to serving API requests will also serve the js files built in the last step.\nFinally, open your browser and navigate to https://\u0026lt;my-hostname\u0026gt;:8081/r/home to display the application\u0026rsquo;s Home page.\nNavigating the Application The Home Page When the application first starts, you can go to the Home page\n\r\rThe initial view of the home page (no logged-in user).\r\r\rThe Application\u0026rsquo;s Menu The application is a SPA with client-side routing and has only a single menu with 5 menu items.\n\r\rThe main menu (no logged-in user).\r\r\r The Home item will take you to the Home page The Users menu item will display the application\u0026rsquo;s Sign-In/Sign-Out page. Here you can connect an identity to your session (log-in), or disconnect an identity from your session (log-out). The Public menu item will request content from an unsecured API endpoint whose content is available to any user whether authenticated or not. The User menu item will request content from an an API endpoint to which access has been restricted to users with role memberships of :admin or :user. The Admin menu item will request content from an an API endpoint to which access has been restricted to users with role membership of :admin.  Access a Public Resource Even though you have not yet signed in, if you click on the Public menu item the application will respond with some content.\n\r\rAccess to Public Resource is allowed (no logged-in user).\r\r\rThis is as expected as that resource is unsecured and available to anyone who can access the application.\nSign-In as the :local/:user User On the Sign-In Page, click on the button labelled \ruser@timpsongray.com. This will associate you session with the application user :user, who has been assigned the :user role. This form of sign-in is a :local authority sign-in. The authority is granted by the application itself.\n\r\rThe Standard Sign-In Page (no logged-in user).\r\r\rOnce you\u0026rsquo;ve done that you\u0026rsquo;ll be redirected to the Home page where your session and identity details are displayed.\n\r\rAfter the user :user has signed in.\r\r\rAccessing a protected resource Now click on the User menu item. The application will attempt to fetch a resource from an API endpoint restricted to users in the :user or :admin roles.\nBecause :user has that role association the contents of the resource is displayed.\n\r\rThe user (:user) is allowed to access to the User resource.\r\r\rHowever, if you now click on the Admin menu item, which attempts to fetch data from an API endpoint restricted to :admin role members only, you\u0026rsquo;ll see an access denied message.\n\r\rThe user (:user) is denied access to the Admin resource.\r\r\rSign-Out from :user Click on the Users menu item to go to the Sign-In/Sign-Out page\n\r\rA view of the standard Sign-Out page with user (:user) is logged in.\r\r\rand at the bottom click on the button in Sign-Out (Local) section. This will remove the identity information from your session, and return you to the Home page.\n\r\rReturned to Home Page after user signs out.\r\r\rSign in as a Google User Again, click on the Users menu item to go to the Sign-In/Sign-Out page\n\r\rThe Standard Sign-In Page.\r\r\rThis time however click on the Google Sign in button. This will open the familiar Google Sign-In dialog where you can login with your Google identity. If the email address of the Google user is registered with an application user ID your session will assigned that identity, but the :authority will now be :google, indicating that is the entity making the assertion of identity.\n\r\rThe Google Sign-In Dialog.\r\r\rAgain, you\u0026rsquo;ll be returned to the Home page where the session\u0026rsquo;s identity information is displayed.\n\r\rThe Home Page with Google signed-in user.\r\r\rBecause \rheykieran@gmail.com is an alias for the user :admin, that is the ID displayed in the top-right corner of the page, and consequently access to the API endpoints restricted to users in the :admin role will be allowed.\nSigning Out If you click on the Users menu item you can return to the Sign-In/Sign-Out page to disconnect your session from the Google account using the Sign Out button. This disconnects your application session, but does not log you out from Google.\n\r\rThe Users page with a Signed-In Google user (mapped to application ID :admin).\r\r\r","date":1588002788,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592760342,"objectID":"37f4baad1635bb85122b7742e828e506","permalink":"https://heykieran.github.io/post/pedestal-and-google/","publishdate":"2020-04-27T11:53:08-04:00","relpermalink":"/post/pedestal-and-google/","section":"post","summary":"A longer discussion of my publicly available GitHub repository containing a secured Pedestal API server and ClojureScript/React SPA that can use Google login to authenticate a user. I show how to set up HTTPS, integrate with Google and secure API endpoints in the context of a simple React application.","tags":["clojure","clojurescript","pedestal","react","security"],"title":"Pedestal API, ClojureScript SPA and Google Authentication","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction \rA repository with some working code and implementation notes can be found here.\r\r\rI had the occasion recently to investigate, and cursorily evaluate, a number of workflow orchestration systems for use on a project with which I was involved. One of those systems, Cadence, particularly appealed to me - there was something very Clojuresque about it; certainly something very suggestive of a functional language.\nIt has the concept of state durability (in workflow functions) that bears more than a passing resemblance to the persistent data structures of Clojure - but extended across time. This concept, similar to checkpoints, opens avenues to consistent, predictable restarts after failures. If one can restore the complete state of a system to a known good-state then one can continue as if the failure had never occurred. Of course, if system-wide (or even better, distributed) non-volatile RAM ever becomes a reality then Cadence would not be needed. This strikes me as essence of the problem Cadence is attempting to solve, or, at least, the gap it\u0026rsquo;s attempting to bridge. Cadence also allows, through activities, the use of non-persistent data structures which can be considered as being analogous to the concept of a side-effect in Clojure.\nThe separation of the functional from the side-effect-ing, and the elision of infrastructure and communication failure concerns leaves developers with simpler, almost always more tractable, domain logic concerns and significantly reduces the cognitive load. This is similar to the benefits often realized through the adoption of functional languages.\nA Brief Tour to Cadence \rCadence is a workflow automation system developed by Uber. It shares many features with other workflow automation systems but differs by being uniquely fault-oblivious rather than merely fault-tolerant. The approach adopted by Cadence simplifies greatly the work of developers who are relieved of many of the burdens of coordinating activities and recovering from system or service failure.\nCadence is complex but three concepts core to its understanding are\n The Cadence Service itself, Workflow Workers and Activity Workers  The Cadence service, backed by a persistent data-store such as Cassandra or MySql, is responsible for orchestrating the activities of both type of workers, for maintaining history, and in the case of failure, for recovering the state of all workflows (but not activities).\nConceptually, the Cadence service instructs a Workflow Worker to execute a Workflow function. The Workflow function, which implements business logic, is guaranteed by Cadence to be durable. That is, its state, including its thread stack and thread-local variables, are known and stored by Cadence, and in the case of failure they are restored.\nWorkflows, like the business processes they typically model, may be long-running. It\u0026rsquo;s not unusual for a real-world business process to take days or even months to complete, and Cadence provides excellent facilities to support such long-running processes within workflow functions. Therefore, the durability of the workflow functions (with the guaranteed recovery of their states across failures) enables a simple straight-line view of the business logic. This greatly reduces the complexity of the development process by reducing the burden on the developer to anticipate and mitigate all failure modes.\nIn order to be able to guarantee durability across failures Cadence places a number of restrictions on the code in Workflow functions. The code must be deterministic i.e. executing the code must produce the same result no matter how often it is run. Therefore, certain actions are forbidden within workflow code - examples being: interacting directly with external services, getting the time, getting random values, and creating or suspending threads.\nThese type of actions are fundamentally non-deterministic and would make full recovery of the workflow state impossible. However, the Cadence API provides alternatives for some of these that produce deterministic behavior; and which assure the recoverability of the function\u0026rsquo;s local variables, threads and state.\nFor situations requiring interaction with external services (the outside world), Cadence insists that all communication be conducted through Activities, using Activity Workers. Activities do not share with Workflows any of Cadence\u0026rsquo;s requirement that they be deterministic. Essentially anything is allowed in activities and any clean-up after failure becomes the responsibility of the developer rather than the Cadence service.\nConceptually, (but not precisely), a Workflow Worker will start an Activity Worker (or multiple Activity Workers) to interact with the outside world. Examples of an Activity might be interacting with a web-service, getting or saving a record to a database, or awaiting human input, such as a decision. Cadence offers no guarantees about activity state, and that state is not recovered in the case of failures of the Cadence infrastructure i.e. within the Cadence service itself.\nIn order to control a running workflow, or to affect its state, it can be signalled using events delivered by Cadence.\nCadence \u0026amp; Clojure Challenges The signature of the worker registration function is registerWorkflowImplementationTypes(java.lang.Class\u0026lt;?\u0026gt;... workflowImplementationClasses) and in the documentation there is the note\n The reason for registration accepting workflow class, but not the workflow instance is that workflows are stateful and a new instance is created for each workflow execution.\n What\u0026rsquo;s not noted, but implied, is that the constructor for the classes must have zero-arg constructors. This is problematic for Clojure as instance variable declared in deftype will create on constructor taking exactly that number of instance variables as arguments.\nYou might then consider inheritance of the deftype-d class to workaround the zero arg constructor issue leaving a cleaner, more Clojure-esque result.\nHowever, although deftype can create a Java class with the fields you need, by default these fields are immutable; but you could use :volatile-mutable to allow the fields to be settable. Unfortunately, the bigger problem is that the generated class is public final which effectively eliminates the possibility that we could use the class as a base class in gen-class.\nThis might have been helpful as we could define a zero-args constructor in gen-class and then using the :constructors field map that constructor to the base class constructor and then assign default values to the field in the :init method. The fact that the deftype-ed class is final eliminates that approach.\nWorking Cadence \u0026amp; Clojure Code In order to fully investigate using Clojure with Cadence I developed a small set of demos to demonstrate how it works, works around what doesn\u0026rsquo;t, and exercises the result. Very little consideration was given to making the code more idiomatic, at least from a Clojure perspective, or even particularly effective. I only making the repository available as it may prove helpful to others who would like to use Clojure with Cadence.\nThe repository also contains further notes on the implementation and lessons learnt.\nWhat\u0026rsquo;s Next? As time allows I\u0026rsquo;ll probably return to the code, making it more idiomatic. But do let me know if you find it helpful, or share your suggestions for improvement.\n","date":1587568238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590701972,"objectID":"ea85603ab754bf4e179277cf351907ed","permalink":"https://heykieran.github.io/post/cadence-and-clojure/","publishdate":"2020-04-22T11:10:38-04:00","relpermalink":"/post/cadence-and-clojure/","section":"post","summary":"How to use Clojure with the Cadence Workflow orchestration system. Some background on Cadence, and links to a working code repository with implementation notes.","tags":["clojure","cadence"],"title":"Cadence Workflow and Clojure","type":"post"},{"authors":["Kieran Owens"],"categories":["Blog Post"],"content":"Introduction\nSetting up Pedestal (using Jetty) with HTTPS isn\u0026rsquo;t that difficult, but it is a bit \u0026ldquo;fiddly\u0026rdquo;. Essentially, you\u0026rsquo;ll need a keystore so that Jetty has access to encryption keys and can encrypt pages sent over HTTPS.\nThis post only deals with self-signed certificates, but if you want to use commercially-signed certificates it should work too.\n\rJust be aware that Jetty is happiest with the pkcs12 format - I\u0026rsquo;ve never got it to work satisfactorily using other formats.\r\r\rService Map (Pedestal)\nIn order to run Jetty under Pedestal you\u0026rsquo;ll need to supply a service map. The following service map works for me. You can change it as you need. The important elements in the current context are where Jetty should look for the keystore (keystore-location), the :ssl? key, the :ssl-port and the :security-provider.\nMake sure the provider (Conscrypt) is in your deps.edn file\n\r(def service-map\r(let\r[keystore-location\r(if (System/getenv \u0026quot;KEYSTORE_LOCATION\u0026quot;)\r(-\u0026gt; (io/file (System/getenv \u0026quot;KEYSTORE_LOCATION\u0026quot;))\r(.getCanonicalPath))\r\u0026quot;/home/user/security/jetty-keystore\u0026quot;)]\r{::http/host \u0026quot;0.0.0.0\u0026quot;\r::http/allowed-origins\r{:allowed-origins (fn[_] true)\r:creds true}\r::http/routes #(deref #'routes)\r::http/type :jetty\r::http/container-options\r{:context-configurator jetty-websocket-configurator\r:h2c? true\r:h2 true\r:ssl? true\r:ssl-port 8081\r:keystore keystore-location\r:key-password \u0026quot;thepassword\u0026quot;\r:security-provider \u0026quot;Conscrypt\u0026quot;}\r::http/port 8080}))\r Jetty Keystore\nIn order for Pedestal to start with Jetty, it expects to find a keystore in a particular location (see Service Map notes above).\nTo create the keystore (I\u0026rsquo;ve plagiarized/assembled from the following pieces of information web, and I\u0026rsquo;m afraid I can\u0026rsquo;t remember the source(s).)\nGenerate a private site key (site.key)\n$ openssl genrsa -des3 -out site.key 2048\r Make a copy of site.key and strip the password, so that it can be auto-loaded\n$ cp site.key site.orig.key\r$ openssl rsa -in site.orig.key -out site.key\r Generate a self-signed signing request (site.csr)\n$ openssl req -new -key site.key -out site.csr\r Generate a self-signed certificate (sitex509.crt - in x509 format for loading into the keystore)\n$ openssl req -new -x509 -key site.key -out sitex509.crt\r Combine the self-signed certificate (sitex509.crt) and site key (site.key) and export it in pkcs12 format (site.pkcs12)\n$ openssl pkcs12 -inkey site.key -in sitex509.crt -export -out site.pkcs12\r Rename the keystore (site.pkcs12) to jetty-keystore\nand adjust the service-map to use it\n","date":1587145177,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590701972,"objectID":"caa521ff1fb0576fd01e59f1ba4c95e2","permalink":"https://heykieran.github.io/post/pedestal-jetty-https/","publishdate":"2020-04-17T13:39:37-04:00","relpermalink":"/post/pedestal-jetty-https/","section":"post","summary":"Introduction\nSetting up Pedestal (using Jetty) with HTTPS isn\u0026rsquo;t that difficult, but it is a bit \u0026ldquo;fiddly\u0026rdquo;. Essentially, you\u0026rsquo;ll need a keystore so that Jetty has access to encryption keys and can encrypt pages sent over HTTPS.","tags":["clojure","pedestal","https"],"title":"Setting-up Pedestal/Jetty with HTTPS","type":"post"}]